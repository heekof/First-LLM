{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d1ef772-429e-4337-83f9-07ebdb13e8d9",
   "metadata": {},
   "source": [
    "### The mathematical trick of self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2d239aec-0517-4a92-a9a2-97964d9ddda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1cbf237-ca4b-4d62-ba4d-eba8caa636e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(69)\n",
    "# We have 8 tokens in a bactch and 4 batches !\n",
    "batch, time, channel = 4,8,2\n",
    "\n",
    "x = torch.randn(batch, time, channel)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a9285f-f3c1-4a23-9f83-9fd94ccd2230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We would like for each token to get the average tokens before per channel, so that we can predict.\n",
    "# Bag Of Words is the term used when we are averaging words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7988b872-c25b-4d3e-a0f5-e49e75bb99a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5300, -1.3035],\n",
       "         [ 0.4438,  1.2221],\n",
       "         [ 1.0395,  0.9608],\n",
       "         [ 0.4214,  0.7452],\n",
       "         [-1.8389, -1.2497],\n",
       "         [-0.2485,  0.1428],\n",
       "         [-1.0509,  0.3527],\n",
       "         [-0.0916,  0.0341]],\n",
       "\n",
       "        [[-0.8986,  0.1022],\n",
       "         [-0.6627, -0.1350],\n",
       "         [-0.3983, -1.7892],\n",
       "         [ 1.2785,  1.3351],\n",
       "         [-0.3066,  1.0382],\n",
       "         [ 1.2762,  0.0419],\n",
       "         [-1.2794, -1.8432],\n",
       "         [ 0.8633, -1.7786]],\n",
       "\n",
       "        [[-0.8080, -0.8735],\n",
       "         [ 0.9367, -1.2319],\n",
       "         [ 1.5287, -0.2759],\n",
       "         [-0.8625, -0.1915],\n",
       "         [-0.4807, -1.4154],\n",
       "         [ 0.0934, -0.2420],\n",
       "         [-1.0300, -0.2034],\n",
       "         [-0.6882, -0.0178]],\n",
       "\n",
       "        [[ 1.1983, -0.8180],\n",
       "         [-0.7297,  0.8256],\n",
       "         [ 0.8756,  0.2960],\n",
       "         [ 0.6394,  1.2406],\n",
       "         [-1.2100, -0.9481],\n",
       "         [ 0.6444,  0.2188],\n",
       "         [-1.2224, -0.9322],\n",
       "         [-0.3832,  1.4027]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size([4, 8, 2])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779f9754-6f21-46d3-a870-5fbc1f5155e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "436ca53e-ef52-43c4-9849-3a1dc2271fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------> Batch number : 0\n",
      "    Time : 0  x[b, :t+1] :\n",
      "tensor([[-0.5300, -1.3035]])\n",
      "\n",
      "    Time : 1  x[b, :t+1] :\n",
      "tensor([[-0.5300, -1.3035],\n",
      "        [ 0.4438,  1.2221]])\n",
      "\n",
      "    Time : 2  x[b, :t+1] :\n",
      "tensor([[-0.5300, -1.3035],\n",
      "        [ 0.4438,  1.2221],\n",
      "        [ 1.0395,  0.9608]])\n",
      "\n",
      "    Time : 3  x[b, :t+1] :\n",
      "tensor([[-0.5300, -1.3035],\n",
      "        [ 0.4438,  1.2221],\n",
      "        [ 1.0395,  0.9608],\n",
      "        [ 0.4214,  0.7452]])\n",
      "\n",
      "    Time : 4  x[b, :t+1] :\n",
      "tensor([[-0.5300, -1.3035],\n",
      "        [ 0.4438,  1.2221],\n",
      "        [ 1.0395,  0.9608],\n",
      "        [ 0.4214,  0.7452],\n",
      "        [-1.8389, -1.2497]])\n",
      "\n",
      "    Time : 5  x[b, :t+1] :\n",
      "tensor([[-0.5300, -1.3035],\n",
      "        [ 0.4438,  1.2221],\n",
      "        [ 1.0395,  0.9608],\n",
      "        [ 0.4214,  0.7452],\n",
      "        [-1.8389, -1.2497],\n",
      "        [-0.2485,  0.1428]])\n",
      "\n",
      "    Time : 6  x[b, :t+1] :\n",
      "tensor([[-0.5300, -1.3035],\n",
      "        [ 0.4438,  1.2221],\n",
      "        [ 1.0395,  0.9608],\n",
      "        [ 0.4214,  0.7452],\n",
      "        [-1.8389, -1.2497],\n",
      "        [-0.2485,  0.1428],\n",
      "        [-1.0509,  0.3527]])\n",
      "\n",
      "    Time : 7  x[b, :t+1] :\n",
      "tensor([[-0.5300, -1.3035],\n",
      "        [ 0.4438,  1.2221],\n",
      "        [ 1.0395,  0.9608],\n",
      "        [ 0.4214,  0.7452],\n",
      "        [-1.8389, -1.2497],\n",
      "        [-0.2485,  0.1428],\n",
      "        [-1.0509,  0.3527],\n",
      "        [-0.0916,  0.0341]])\n",
      "\n",
      "-------> Batch number : 1\n",
      "    Time : 0  x[b, :t+1] :\n",
      "tensor([[-0.8986,  0.1022]])\n",
      "\n",
      "    Time : 1  x[b, :t+1] :\n",
      "tensor([[-0.8986,  0.1022],\n",
      "        [-0.6627, -0.1350]])\n",
      "\n",
      "    Time : 2  x[b, :t+1] :\n",
      "tensor([[-0.8986,  0.1022],\n",
      "        [-0.6627, -0.1350],\n",
      "        [-0.3983, -1.7892]])\n",
      "\n",
      "    Time : 3  x[b, :t+1] :\n",
      "tensor([[-0.8986,  0.1022],\n",
      "        [-0.6627, -0.1350],\n",
      "        [-0.3983, -1.7892],\n",
      "        [ 1.2785,  1.3351]])\n",
      "\n",
      "    Time : 4  x[b, :t+1] :\n",
      "tensor([[-0.8986,  0.1022],\n",
      "        [-0.6627, -0.1350],\n",
      "        [-0.3983, -1.7892],\n",
      "        [ 1.2785,  1.3351],\n",
      "        [-0.3066,  1.0382]])\n",
      "\n",
      "    Time : 5  x[b, :t+1] :\n",
      "tensor([[-0.8986,  0.1022],\n",
      "        [-0.6627, -0.1350],\n",
      "        [-0.3983, -1.7892],\n",
      "        [ 1.2785,  1.3351],\n",
      "        [-0.3066,  1.0382],\n",
      "        [ 1.2762,  0.0419]])\n",
      "\n",
      "    Time : 6  x[b, :t+1] :\n",
      "tensor([[-0.8986,  0.1022],\n",
      "        [-0.6627, -0.1350],\n",
      "        [-0.3983, -1.7892],\n",
      "        [ 1.2785,  1.3351],\n",
      "        [-0.3066,  1.0382],\n",
      "        [ 1.2762,  0.0419],\n",
      "        [-1.2794, -1.8432]])\n",
      "\n",
      "    Time : 7  x[b, :t+1] :\n",
      "tensor([[-0.8986,  0.1022],\n",
      "        [-0.6627, -0.1350],\n",
      "        [-0.3983, -1.7892],\n",
      "        [ 1.2785,  1.3351],\n",
      "        [-0.3066,  1.0382],\n",
      "        [ 1.2762,  0.0419],\n",
      "        [-1.2794, -1.8432],\n",
      "        [ 0.8633, -1.7786]])\n",
      "\n",
      "-------> Batch number : 2\n",
      "    Time : 0  x[b, :t+1] :\n",
      "tensor([[-0.8080, -0.8735]])\n",
      "\n",
      "    Time : 1  x[b, :t+1] :\n",
      "tensor([[-0.8080, -0.8735],\n",
      "        [ 0.9367, -1.2319]])\n",
      "\n",
      "    Time : 2  x[b, :t+1] :\n",
      "tensor([[-0.8080, -0.8735],\n",
      "        [ 0.9367, -1.2319],\n",
      "        [ 1.5287, -0.2759]])\n",
      "\n",
      "    Time : 3  x[b, :t+1] :\n",
      "tensor([[-0.8080, -0.8735],\n",
      "        [ 0.9367, -1.2319],\n",
      "        [ 1.5287, -0.2759],\n",
      "        [-0.8625, -0.1915]])\n",
      "\n",
      "    Time : 4  x[b, :t+1] :\n",
      "tensor([[-0.8080, -0.8735],\n",
      "        [ 0.9367, -1.2319],\n",
      "        [ 1.5287, -0.2759],\n",
      "        [-0.8625, -0.1915],\n",
      "        [-0.4807, -1.4154]])\n",
      "\n",
      "    Time : 5  x[b, :t+1] :\n",
      "tensor([[-0.8080, -0.8735],\n",
      "        [ 0.9367, -1.2319],\n",
      "        [ 1.5287, -0.2759],\n",
      "        [-0.8625, -0.1915],\n",
      "        [-0.4807, -1.4154],\n",
      "        [ 0.0934, -0.2420]])\n",
      "\n",
      "    Time : 6  x[b, :t+1] :\n",
      "tensor([[-0.8080, -0.8735],\n",
      "        [ 0.9367, -1.2319],\n",
      "        [ 1.5287, -0.2759],\n",
      "        [-0.8625, -0.1915],\n",
      "        [-0.4807, -1.4154],\n",
      "        [ 0.0934, -0.2420],\n",
      "        [-1.0300, -0.2034]])\n",
      "\n",
      "    Time : 7  x[b, :t+1] :\n",
      "tensor([[-0.8080, -0.8735],\n",
      "        [ 0.9367, -1.2319],\n",
      "        [ 1.5287, -0.2759],\n",
      "        [-0.8625, -0.1915],\n",
      "        [-0.4807, -1.4154],\n",
      "        [ 0.0934, -0.2420],\n",
      "        [-1.0300, -0.2034],\n",
      "        [-0.6882, -0.0178]])\n",
      "\n",
      "-------> Batch number : 3\n",
      "    Time : 0  x[b, :t+1] :\n",
      "tensor([[ 1.1983, -0.8180]])\n",
      "\n",
      "    Time : 1  x[b, :t+1] :\n",
      "tensor([[ 1.1983, -0.8180],\n",
      "        [-0.7297,  0.8256]])\n",
      "\n",
      "    Time : 2  x[b, :t+1] :\n",
      "tensor([[ 1.1983, -0.8180],\n",
      "        [-0.7297,  0.8256],\n",
      "        [ 0.8756,  0.2960]])\n",
      "\n",
      "    Time : 3  x[b, :t+1] :\n",
      "tensor([[ 1.1983, -0.8180],\n",
      "        [-0.7297,  0.8256],\n",
      "        [ 0.8756,  0.2960],\n",
      "        [ 0.6394,  1.2406]])\n",
      "\n",
      "    Time : 4  x[b, :t+1] :\n",
      "tensor([[ 1.1983, -0.8180],\n",
      "        [-0.7297,  0.8256],\n",
      "        [ 0.8756,  0.2960],\n",
      "        [ 0.6394,  1.2406],\n",
      "        [-1.2100, -0.9481]])\n",
      "\n",
      "    Time : 5  x[b, :t+1] :\n",
      "tensor([[ 1.1983, -0.8180],\n",
      "        [-0.7297,  0.8256],\n",
      "        [ 0.8756,  0.2960],\n",
      "        [ 0.6394,  1.2406],\n",
      "        [-1.2100, -0.9481],\n",
      "        [ 0.6444,  0.2188]])\n",
      "\n",
      "    Time : 6  x[b, :t+1] :\n",
      "tensor([[ 1.1983, -0.8180],\n",
      "        [-0.7297,  0.8256],\n",
      "        [ 0.8756,  0.2960],\n",
      "        [ 0.6394,  1.2406],\n",
      "        [-1.2100, -0.9481],\n",
      "        [ 0.6444,  0.2188],\n",
      "        [-1.2224, -0.9322]])\n",
      "\n",
      "    Time : 7  x[b, :t+1] :\n",
      "tensor([[ 1.1983, -0.8180],\n",
      "        [-0.7297,  0.8256],\n",
      "        [ 0.8756,  0.2960],\n",
      "        [ 0.6394,  1.2406],\n",
      "        [-1.2100, -0.9481],\n",
      "        [ 0.6444,  0.2188],\n",
      "        [-1.2224, -0.9322],\n",
      "        [-0.3832,  1.4027]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xbow = torch.zeros((batch, time, channel))\n",
    "for b in range(batch):\n",
    "    print('-------> Batch number :', b)\n",
    "    for t in range(time):\n",
    "        print('    Time :', t, ' x[b, :t+1] :')\n",
    "        xprev = x[b, :t+1]\n",
    "        print(xprev)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d983ced8-fcf4-4f2a-996c-b01e7d510da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5300, -1.3035],\n",
       "        [ 0.4438,  1.2221],\n",
       "        [ 1.0395,  0.9608],\n",
       "        [ 0.4214,  0.7452],\n",
       "        [-1.8389, -1.2497],\n",
       "        [-0.2485,  0.1428],\n",
       "        [-1.0509,  0.3527],\n",
       "        [-0.0916,  0.0341]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ec542f1-ae08-4785-a962-59c91f6d7219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1983, -0.8180],\n",
       "        [-0.7297,  0.8256],\n",
       "        [ 0.8756,  0.2960],\n",
       "        [ 0.6394,  1.2406],\n",
       "        [-1.2100, -0.9481],\n",
       "        [ 0.6444,  0.2188],\n",
       "        [-1.2224, -0.9322],\n",
       "        [-0.3832,  1.4027]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[b, :t+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d61bc21-72ab-45c2-8912-d81c948ae001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1983, -0.8180])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xprev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4546c51-88af-49aa-82a9-889a54822d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0234,  0.1606])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc6bffb1-906b-41c8-b0ee-9cdeb9ede2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b0d4eb5-01aa-413b-ac7c-5cca43a5fa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 5.],\n",
      "        [9., 3.],\n",
      "        [2., 5.]])\n",
      "tensor([[11., 13.],\n",
      "        [11., 13.],\n",
      "        [11., 13.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(69)\n",
    "a = torch.ones((3,3))\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "print(a)\n",
    "print(b)\n",
    "print(a @ b) # dot product, produit scalaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "633f4008-ca54-4f26-b75d-7892b1a73180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  You can create a cumulate sum using this trorch.tril trick !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5f732837-3b1b-4d6f-bf8a-8b57baa40bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 5.],\n",
      "        [9., 3.],\n",
      "        [2., 5.]])\n",
      "tensor([[ 0.,  5.],\n",
      "        [ 9.,  8.],\n",
      "        [11., 13.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(69)\n",
    "a = torch.tril(torch.ones((3,3)))\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "print(a)\n",
    "print(b)\n",
    "print(a @ b) # dot product, produit scalaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1ef370c-3363-46bb-a23c-8919da159f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3., 2., 1.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(a)\n",
    "torch.sum(a,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "15c7c692-c5ea-46be-8b90-0bef9d4ddcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3., 2., 1.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(a)\n",
    "torch.sum(a,0, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6770dd4c-b9b6-4914-961b-ba4f41716f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(a)\n",
    "torch.sum(a,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f4fb8c-2a85-4b7e-962a-624d8e07132d",
   "metadata": {},
   "source": [
    "|-----|-----|-----|\n",
    "| a11 | a12 | a13 | (Row 1)\n",
    "|-----|-----|-----|\n",
    "| a21 | a22 | a23 | (Row 2)\n",
    "\n",
    "torch.sum(matrix, 0): Summing Across Rows\n",
    "\n",
    "| a11 + a21 | a12 + a22 | a13 + a23 | (Sum of each column)\n",
    "\n",
    "\n",
    "torch.sum(matrix, 1): Summing Across Columns\n",
    "\n",
    "| a11 + a12 + a13 | (Sum of elements in Row 1)\n",
    "| a21 + a22 + a23 | (Sum of elements in Row 2)\n",
    "\n",
    "\n",
    "- Setting dim=0 sums across rows, while dim=1 sums across columns.\n",
    "- The output tensor's shape is affected based on the specified dimension.\n",
    "\n",
    "Summing over axis 0 collapses the matrix vertically, producing a sum of elements column-wise.\n",
    "Summing over axis 1 collapses the matrix horizontally, resulting in a sum of elements row-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d26e0a38-1e81-4200-9778-18b014493141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 7, 9]])\n",
      "tensor([5, 7, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Sum across rows, keeping the 0th dimension (1x3)\n",
    "sum_with_dim = torch.sum(a, 0, keepdim=True)\n",
    "print(sum_with_dim)  # Output: tensor([[5, 7, 9]])\n",
    "\n",
    "# Sum across rows, removing the 0th dimension (3)\n",
    "sum_without_dim = torch.sum(a, 0, keepdim=False)\n",
    "print(sum_without_dim)  # Output: tensor([5, 7, 9])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcadd14c-ad5b-42b7-b0cc-589ceca7c987",
   "metadata": {},
   "source": [
    "Using keepdim=True is particularly useful when you need to perform further operations that require the input dimensions to remain consistent, such as when you want to divide by the sum or apply some weighted operation while preserving the original tensor shape for broadcasting compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9132b70f-627a-4874-bb5a-12cdbf684454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[0., 5.],\n",
      "        [9., 3.],\n",
      "        [2., 5.]])\n",
      "tensor([[0.0000, 5.0000],\n",
      "        [4.5000, 4.0000],\n",
      "        [3.6667, 4.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(69)\n",
    "a = torch.tril(torch.ones((3,3)))\n",
    "a = a / torch.sum(a, 1, keepdim = True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "print(a)\n",
    "print(b)\n",
    "print(a @ b) # dot product, produit scalaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c95fbb-7bc6-4991-8c12-0b6792b4e1b2",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3f0dcee2-a619-4e8a-bca2-4404f2edf55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(69)\n",
    "# We have 8 tokens in a bactch and 4 batches !\n",
    "batch, time, channel = 4,8,2\n",
    "\n",
    "x = torch.randn(batch, time, channel)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6467b45d-72e7-4441-9f2e-c33dc1c91515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5300, -1.3035],\n",
       "         [ 0.4438,  1.2221],\n",
       "         [ 1.0395,  0.9608],\n",
       "         [ 0.4214,  0.7452],\n",
       "         [-1.8389, -1.2497],\n",
       "         [-0.2485,  0.1428],\n",
       "         [-1.0509,  0.3527],\n",
       "         [-0.0916,  0.0341]],\n",
       "\n",
       "        [[-0.8986,  0.1022],\n",
       "         [-0.6627, -0.1350],\n",
       "         [-0.3983, -1.7892],\n",
       "         [ 1.2785,  1.3351],\n",
       "         [-0.3066,  1.0382],\n",
       "         [ 1.2762,  0.0419],\n",
       "         [-1.2794, -1.8432],\n",
       "         [ 0.8633, -1.7786]],\n",
       "\n",
       "        [[-0.8080, -0.8735],\n",
       "         [ 0.9367, -1.2319],\n",
       "         [ 1.5287, -0.2759],\n",
       "         [-0.8625, -0.1915],\n",
       "         [-0.4807, -1.4154],\n",
       "         [ 0.0934, -0.2420],\n",
       "         [-1.0300, -0.2034],\n",
       "         [-0.6882, -0.0178]],\n",
       "\n",
       "        [[ 1.1983, -0.8180],\n",
       "         [-0.7297,  0.8256],\n",
       "         [ 0.8756,  0.2960],\n",
       "         [ 0.6394,  1.2406],\n",
       "         [-1.2100, -0.9481],\n",
       "         [ 0.6444,  0.2188],\n",
       "         [-1.2224, -0.9322],\n",
       "         [-0.3832,  1.4027]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "acbd4a97-c128-48a2-9f9d-d91d28f53b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tril(torch.ones(time, time))\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f8df497-7747-4b2f-9ad0-b3f82a0a2832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.],\n",
       "        [7.],\n",
       "        [8.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_weights = torch.sum(weights,1, keepdim=True)\n",
    "sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d203a1ab-d9f9-4cd9-949f-bcddacbb6077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_weights = weights/sum_weights\n",
    "average_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f4ed2dfe-10b0-41a9-8ee2-b8afbb522ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0d84c05b-35bd-41e5-b3e8-ef84777baed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e6fed85e-acd4-41d1-ab5b-533ffc0d4f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "x.shape is torch.Size([4, 8, 2])\n",
    "\n",
    "average_weight.shape is torch.Size([8, 8])\n",
    "\n",
    "What will happen is I do a dot multiply in Pytorch ? explain the logic\n",
    "\n",
    "\n",
    "For matrix multiplication, the inner dimensions must match. In simpler terms, if you're multiplying matrices A and B where A is of shape (m, n) and B is of shape (n, p), the result will be of shape (m, p).\n",
    "\n",
    "\n",
    "( time, time ) @ (batch, time, channel) ==> in Pytorch  : (batch, time, time) @ (batch, time, channel) => (batch, time, channel)\n",
    "\"\"\"\n",
    "(average_weights @ x).shape\n",
    "#torch.Size([4, 8, 2])\n",
    "\"\"\"\n",
    "Given the result you've provided, it appears there's been a misunderstanding on my part regarding the handling of the operation given \n",
    "the tensor shapes in PyTorch. Let's clarify the correct behavior and logic behind it:\n",
    "\n",
    "- `x.shape`: `[4, 8, 2]`\n",
    "- `average_weights.shape`: `[8, 8]`\n",
    "\n",
    "And the operation `average_weights @ x` gives a result of shape `[4, 8, 2]`.\n",
    "\n",
    "Here's the corrected explanation based on the operation and its result:\n",
    "\n",
    "1. **Tensor Broadcasting**: PyTorch allows broadcasting of the `average_weights` tensor across the batch dimension of `x` implicitly \n",
    "      during the matrix multiplication operation. This means `average_weights` is applied to each of the 4 matrices in `x` as \n",
    "      if it were a batch operation, even though `average_weights` doesn't explicitly have a batch dimension.\n",
    "\n",
    "2. **Matrix Multiplication Logic**:\n",
    "   - The operation treats `x` as a batch of 4 matrices of size `[8, 2]`.\n",
    "   - `average_weights`, being an `[8, 8]` matrix, is compatible for multiplication with each `[8, 2]` matrix in `x`.\n",
    "   - For each of the 4 matrices in `x`, the `average_weights` matrix is multiplied, resulting in 4 matrices of shape `[8, 2]`.\n",
    "\n",
    "3. **Result Shape**: The operation effectively performs the matrix multiplication for each matrix in the batch separately, \n",
    "        maintaining the batch size (4) and the resultant matrix shape `[8, 2]` for each multiplication. \n",
    "        Hence, the overall result shape is `[4, 8, 2]`, which matches the shape you've observed.\n",
    "\n",
    "This corrected explanation accounts for the behavior of PyTorch's matrix multiplication with broadcasting, \n",
    "where `average_weights` is applied to each item in the batch of `x`, yielding the result with the same batch dimension and \n",
    "the expected shape after the multiplication.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8454e3f8-6f1a-46d6-86a9-a8a0bdbbe9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "   [8, 8] @ [4, 8, 2]\n",
    "\n",
    "[4, 8, 8] @ [4, 8, 2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8a400b28-2ff5-49f6-af8b-a26b4ff27108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5300, -1.3035],\n",
       "         [ 0.4438,  1.2221],\n",
       "         [ 1.0395,  0.9608],\n",
       "         [ 0.4214,  0.7452],\n",
       "         [-1.8389, -1.2497],\n",
       "         [-0.2485,  0.1428],\n",
       "         [-1.0509,  0.3527],\n",
       "         [-0.0916,  0.0341]],\n",
       "\n",
       "        [[-0.8986,  0.1022],\n",
       "         [-0.6627, -0.1350],\n",
       "         [-0.3983, -1.7892],\n",
       "         [ 1.2785,  1.3351],\n",
       "         [-0.3066,  1.0382],\n",
       "         [ 1.2762,  0.0419],\n",
       "         [-1.2794, -1.8432],\n",
       "         [ 0.8633, -1.7786]],\n",
       "\n",
       "        [[-0.8080, -0.8735],\n",
       "         [ 0.9367, -1.2319],\n",
       "         [ 1.5287, -0.2759],\n",
       "         [-0.8625, -0.1915],\n",
       "         [-0.4807, -1.4154],\n",
       "         [ 0.0934, -0.2420],\n",
       "         [-1.0300, -0.2034],\n",
       "         [-0.6882, -0.0178]],\n",
       "\n",
       "        [[ 1.1983, -0.8180],\n",
       "         [-0.7297,  0.8256],\n",
       "         [ 0.8756,  0.2960],\n",
       "         [ 0.6394,  1.2406],\n",
       "         [-1.2100, -0.9481],\n",
       "         [ 0.6444,  0.2188],\n",
       "         [-1.2224, -0.9322],\n",
       "         [-0.3832,  1.4027]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "32c34eb4-4ebe-41c8-bd05-0222948e073f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5300, -1.3035],\n",
       "         [-0.0431, -0.0407],\n",
       "         [ 0.3178,  0.2932],\n",
       "         [ 0.3437,  0.4062],\n",
       "         [-0.0928,  0.0750],\n",
       "         [-0.1188,  0.0863],\n",
       "         [-0.2519,  0.1244],\n",
       "         [-0.2319,  0.1131]],\n",
       "\n",
       "        [[-0.8986,  0.1022],\n",
       "         [-0.7807, -0.0164],\n",
       "         [-0.6532, -0.6073],\n",
       "         [-0.1703, -0.1217],\n",
       "         [-0.1976,  0.1103],\n",
       "         [ 0.0481,  0.0989],\n",
       "         [-0.1416, -0.1786],\n",
       "         [-0.0160, -0.3786]],\n",
       "\n",
       "        [[-0.8080, -0.8735],\n",
       "         [ 0.0643, -1.0527],\n",
       "         [ 0.5525, -0.7937],\n",
       "         [ 0.1987, -0.6432],\n",
       "         [ 0.0628, -0.7976],\n",
       "         [ 0.0679, -0.7050],\n",
       "         [-0.0889, -0.6334],\n",
       "         [-0.1638, -0.5564]],\n",
       "\n",
       "        [[ 1.1983, -0.8180],\n",
       "         [ 0.2343,  0.0038],\n",
       "         [ 0.4481,  0.1012],\n",
       "         [ 0.4959,  0.3860],\n",
       "         [ 0.1547,  0.1192],\n",
       "         [ 0.2363,  0.1358],\n",
       "         [ 0.0279, -0.0168],\n",
       "         [-0.0234,  0.1606]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_weights @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8f5dcc5d-cbd3-4cde-a085-74964f609e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5300, -1.3035],\n",
       "         [-0.0431, -0.0407],\n",
       "         [ 0.3178,  0.2932],\n",
       "         [ 0.3437,  0.4062],\n",
       "         [-0.0928,  0.0750],\n",
       "         [-0.1188,  0.0863],\n",
       "         [-0.2519,  0.1244],\n",
       "         [-0.2319,  0.1131]],\n",
       "\n",
       "        [[-0.8986,  0.1022],\n",
       "         [-0.7807, -0.0164],\n",
       "         [-0.6532, -0.6073],\n",
       "         [-0.1703, -0.1217],\n",
       "         [-0.1976,  0.1103],\n",
       "         [ 0.0481,  0.0989],\n",
       "         [-0.1416, -0.1786],\n",
       "         [-0.0160, -0.3786]],\n",
       "\n",
       "        [[-0.8080, -0.8735],\n",
       "         [ 0.0643, -1.0527],\n",
       "         [ 0.5525, -0.7937],\n",
       "         [ 0.1987, -0.6432],\n",
       "         [ 0.0628, -0.7976],\n",
       "         [ 0.0679, -0.7050],\n",
       "         [-0.0889, -0.6334],\n",
       "         [-0.1638, -0.5564]],\n",
       "\n",
       "        [[ 1.1983, -0.8180],\n",
       "         [ 0.2343,  0.0038],\n",
       "         [ 0.4481,  0.1012],\n",
       "         [ 0.4959,  0.3860],\n",
       "         [ 0.1547,  0.1192],\n",
       "         [ 0.2363,  0.1358],\n",
       "         [ 0.0279, -0.0168],\n",
       "         [-0.0234,  0.1606]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "17d5ad01-30e1-479a-9256-42bd584ea3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ee519d40-41f0-41b1-9e35-db1e5ccb0cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[False,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False, False,  True,  True,  True,  True,  True],\n",
       "         [False, False, False, False,  True,  True,  True,  True],\n",
       "         [False, False, False, False, False,  True,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False, False,  True],\n",
       "         [False, False, False, False, False, False, False, False]]),)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril == 0,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "82ad6ebb-7a81-49e1-abeb-a55e1b8eaeb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[False,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False, False,  True,  True,  True,  True,  True],\n",
       "         [False, False, False, False,  True,  True,  True,  True],\n",
       "         [False, False, False, False, False,  True,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False, False,  True],\n",
       "         [False, False, False, False, False, False, False, False]]),)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril[:time, :time] == 0,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e8f05887-ccbb-4664-bd33-5ba9d5aab97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(time, time))\n",
    "weights = torch.zeros(time, time)\n",
    "# Don't look at the future so set it to -inf\n",
    "weights = weights.masked_fill(tril[:time, :time] == 0, float('-inf')) # (Batch, Time, Time) \n",
    "# softmax exp(-inf) = 0, exp(0) = 1 and then softmax on dim=1 (across row) with sum up the Zeros\n",
    "weights = F.softmax(weights, dim=-1) # (B, T, T)\n",
    "xbow3 = weights @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "76b09ff4-032a-43ed-b2da-5983a762aa42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a644438-0cec-49c8-9205-2643e7c7564c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64380778-9410-42b6-94bb-e86c02aa33e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fc8fd3-f06c-47b4-ba29-8ba989f7cba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ed417-e7a1-4ceb-9f4b-2de09bf887d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ce52b1-e1a1-4f6e-871f-6d59fa4f26fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start-of-sentence', 'object', 'that', 'has', 'a', 'no-return', 'boundary', 'for', 'other', 'uses', 'see', 'black', 'hole', 'disambiguation', 'end-of-sentence', 'start-of-sentence', 'direct', 'radio', 'image', 'of', 'a', 'supermassive', 'black', 'hole', 'at', 'the', 'core', 'of', 'messier', '87', 'animated', 'simulation', 'of', 'a', 'schwarzschild', 'black', 'hole', 'with', 'a', 'galaxy', 'passing', 'behind', 'end-of-sentence', 'start-of-sentence', 'around', 'the', 'time', 'of', 'alignment', 'extreme', 'gravitational', 'lensing', 'of', 'the', 'galaxy', 'is', 'observed', 'end-of-sentence', 'start-of-sentence', 'a', 'black', 'hole', 'is', 'a', 'region', 'of', 'spacetime', 'where', 'gravity', 'is', 'so', 'strong', 'that', 'nothing', 'including', 'light', 'and', 'other', 'electromagnetic', 'waves', 'has', 'enough', 'energy', 'to', 'escape', 'it', 'end-of-sentence', 'start-of-sentence', 'einstein', 's', 'theory', 'of', 'general', 'relativity', 'predicts', 'that', 'a', 'sufficiently', 'compact', 'mass']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "import random\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "file_path = '/Users/jaafarbendriss/First-LLM/dataset/black_hole_wiki.txt'\n",
    "\n",
    "# Open the file and read its content\n",
    "with open(file_path, 'r') as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "raw_text = file_content\n",
    "\n",
    "def add_start_end_sentence(text):\n",
    "    return \"start-of-sentence \" + text.replace(\".\",\" end-of-sentence start-of-sentence\")\n",
    "\n",
    "\n",
    "def remove_citations(text):\n",
    "    pattern = r\"\\[\\d+\\]\"\n",
    "    cleaned_text = re.sub(pattern, \"\", text)    \n",
    "    return cleaned_text\n",
    "\n",
    "def remove_spaces(text):\n",
    "    pattern = r\"\\s+\"\n",
    "    cleaned_text = re.sub(pattern, \" \", text)    \n",
    "    return cleaned_text\n",
    "\n",
    "def preprocess(text: str):\n",
    "    return remove_spaces(remove_citations(add_start_end_sentence(text).lower().replace(\"(\",\"\").replace(\")\",\"\").replace(\" s \",\"\").replace(\"\\\\\",\"\").replace(\"'\",\" \").replace('\"',' ').replace(\",\",\" \").replace(\"\\ s\",\" \").replace(\"\\n\",\" \")))\n",
    "\n",
    "preprocessed_text = preprocess(raw_text)\n",
    "\n",
    "Counter(preprocessed_text.split(\" \")).most_common(20)\n",
    "\n",
    "preprocessed_text_array = preprocessed_text.split(\" \")\n",
    "\n",
    "\n",
    "print(preprocessed_text_array[0:100])\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "tokens = tokenizer(preprocessed_text)\n",
    "\n",
    "preprocessed_text = preprocessed_text.replace(\"end-of-sentence start-of-sentence\", \"end-of-sentence69696969start-of-sentence\")\n",
    "splited_data = preprocessed_text.split(\"69696969\")\n",
    "\n",
    "train_data =  [(i,sentence) for i, sentence in zip( range(0,len(splited_data)), splited_data  )]\n",
    "train_iter = iter(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd3b9dd7-bcaa-4636-ab54-77feeea353b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de2e6f-6068-4eaf-bd11-c50c60b2aac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b653746-6214-4009-9a0d-bad541481b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " 'start-of-sentence preface a chancery judge once had the kindness to inform me as one of a company of some hundred and fifty men and women not labouring under any suspicions of lunacy that the court of chancery though the shining subject of much popular prejudice at which point i thought the judge’s eye had a cast in my direction was almost immaculate end-of-sentence')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df63202c-3e25-4118-969e-4b545e05d36e",
   "metadata": {},
   "source": [
    "## Preparing training dataset using sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd6ae785-37ca-4958-97a2-1abe6e9310ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "\n",
    "    #_list_of_sentences = None\n",
    "    \n",
    "    def __init__(self, list_of_sentences):\n",
    "        self._list_of_sentences = list_of_sentences\n",
    "        self.tokens = dict()\n",
    "\n",
    "    def compute_tokens(self):\n",
    "        all_words = set([w for stc in self._list_of_sentences for w in stc ])\n",
    "\n",
    "        index = 0\n",
    "        \n",
    "        for sentence in self._list_of_sentences:\n",
    "            for word in sentence.split(\" \"):\n",
    "                if word not in self.tokens:\n",
    "                    self.tokens[word] = index\n",
    "                    index += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b3f75cd-d02f-4f2c-aaed-791c61fe2d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3731"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splited_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c34fdb1-03bc-4072-a147-f00bb8ec7b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3731"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVocab = Vocab(splited_data)\n",
    "\n",
    "len(myVocab._list_of_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f418f13d-b11f-4ada-a201-fde5fa37c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "myVocab.compute_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05a0d76b-0771-4730-aa02-24ecc484286c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start-of-sentence': 0,\n",
       " 'preface': 1,\n",
       " 'a': 2,\n",
       " 'chancery': 3,\n",
       " 'judge': 4,\n",
       " 'once': 5,\n",
       " 'had': 6,\n",
       " 'the': 7,\n",
       " 'kindness': 8,\n",
       " 'to': 9,\n",
       " 'inform': 10,\n",
       " 'me': 11,\n",
       " 'as': 12,\n",
       " 'one': 13,\n",
       " 'of': 14,\n",
       " 'company': 15,\n",
       " 'some': 16,\n",
       " 'hundred': 17,\n",
       " 'and': 18,\n",
       " 'fifty': 19,\n",
       " 'men': 20,\n",
       " 'women': 21,\n",
       " 'not': 22,\n",
       " 'labouring': 23,\n",
       " 'under': 24,\n",
       " 'any': 25,\n",
       " 'suspicions': 26,\n",
       " 'lunacy': 27,\n",
       " 'that': 28,\n",
       " 'court': 29,\n",
       " 'though': 30,\n",
       " 'shining': 31,\n",
       " 'subject': 32,\n",
       " 'much': 33,\n",
       " 'popular': 34,\n",
       " 'prejudice': 35,\n",
       " 'at': 36,\n",
       " 'which': 37,\n",
       " 'point': 38,\n",
       " 'i': 39,\n",
       " 'thought': 40,\n",
       " 'judge’s': 41,\n",
       " 'eye': 42,\n",
       " 'cast': 43,\n",
       " 'in': 44,\n",
       " 'my': 45,\n",
       " 'direction': 46,\n",
       " 'was': 47,\n",
       " 'almost': 48,\n",
       " 'immaculate': 49,\n",
       " 'end-of-sentence': 50,\n",
       " 'there': 51,\n",
       " 'been': 52,\n",
       " 'he': 53,\n",
       " 'admitted': 54,\n",
       " 'trivial': 55,\n",
       " 'blemish': 56,\n",
       " 'or': 57,\n",
       " 'so': 58,\n",
       " 'its': 59,\n",
       " 'rate': 60,\n",
       " 'progress': 61,\n",
       " 'but': 62,\n",
       " 'this': 63,\n",
       " 'exaggerated': 64,\n",
       " 'entirely': 65,\n",
       " 'owing': 66,\n",
       " '“parsimony': 67,\n",
       " 'public': 68,\n",
       " '”': 69,\n",
       " 'guilty': 70,\n",
       " 'it': 71,\n",
       " 'appeared': 72,\n",
       " 'until': 73,\n",
       " 'lately': 74,\n",
       " 'bent': 75,\n",
       " 'most': 76,\n",
       " 'determined': 77,\n",
       " 'manner': 78,\n",
       " 'on': 79,\n",
       " 'by': 80,\n",
       " 'no': 81,\n",
       " 'means': 82,\n",
       " 'enlarging': 83,\n",
       " 'number': 84,\n",
       " 'judges': 85,\n",
       " 'appointed—i': 86,\n",
       " 'believe': 87,\n",
       " 'richard': 88,\n",
       " 'second': 89,\n",
       " 'other': 90,\n",
       " 'king': 91,\n",
       " 'will': 92,\n",
       " 'do': 93,\n",
       " 'well': 94,\n",
       " 'seemed': 95,\n",
       " 'too': 96,\n",
       " 'profound': 97,\n",
       " 'joke': 98,\n",
       " 'be': 99,\n",
       " 'inserted': 100,\n",
       " 'body': 101,\n",
       " 'book': 102,\n",
       " 'should': 103,\n",
       " 'have': 104,\n",
       " 'restored': 105,\n",
       " 'conversation': 106,\n",
       " 'kenge': 107,\n",
       " 'mr': 108,\n",
       " 'vholes': 109,\n",
       " 'with': 110,\n",
       " 'whom': 111,\n",
       " 'think': 112,\n",
       " 'must': 113,\n",
       " 'originated': 114,\n",
       " 'such': 115,\n",
       " 'mouths': 116,\n",
       " 'might': 117,\n",
       " 'coupled': 118,\n",
       " 'an': 119,\n",
       " 'apt': 120,\n",
       " 'quotation': 121,\n",
       " 'from': 122,\n",
       " 'shakespeare’s': 123,\n",
       " 'sonnets:': 124,\n",
       " '“my': 125,\n",
       " 'nature': 126,\n",
       " 'is': 127,\n",
       " 'subdued': 128,\n",
       " 'what': 129,\n",
       " 'works': 130,\n",
       " 'like': 131,\n",
       " 'dyer’s': 132,\n",
       " 'hand:': 133,\n",
       " 'pity': 134,\n",
       " 'then': 135,\n",
       " 'wish': 136,\n",
       " 'were': 137,\n",
       " 'renewed!”': 138,\n",
       " 'wholesome': 139,\n",
       " 'parsimonious': 140,\n",
       " 'know': 141,\n",
       " 'has': 142,\n",
       " 'doing': 143,\n",
       " 'still': 144,\n",
       " 'connexion': 145,\n",
       " 'mention': 146,\n",
       " 'here': 147,\n",
       " 'everything': 148,\n",
       " 'set': 149,\n",
       " 'forth': 150,\n",
       " 'these': 151,\n",
       " 'pages': 152,\n",
       " 'concerning': 153,\n",
       " 'substantially': 154,\n",
       " 'true': 155,\n",
       " 'within': 156,\n",
       " 'truth': 157,\n",
       " 'case': 158,\n",
       " 'gridley': 159,\n",
       " 'essential': 160,\n",
       " 'altered': 161,\n",
       " 'actual': 162,\n",
       " 'occurrence': 163,\n",
       " 'made': 164,\n",
       " 'disinterested': 165,\n",
       " 'person': 166,\n",
       " 'who': 167,\n",
       " 'professionally': 168,\n",
       " 'acquainted': 169,\n",
       " 'whole': 170,\n",
       " 'monstrous': 171,\n",
       " 'wrong': 172,\n",
       " 'beginning': 173,\n",
       " 'end': 174,\n",
       " 'present': 175,\n",
       " 'moment': 176,\n",
       " 'august': 177,\n",
       " '1853': 178,\n",
       " 'suit': 179,\n",
       " 'before': 180,\n",
       " 'commenced': 181,\n",
       " 'nearly': 182,\n",
       " 'twenty': 183,\n",
       " 'years': 184,\n",
       " 'ago': 185,\n",
       " 'thirty': 186,\n",
       " 'forty': 187,\n",
       " 'counsel': 188,\n",
       " 'known': 189,\n",
       " 'appear': 190,\n",
       " 'time': 191,\n",
       " 'costs': 192,\n",
       " 'incurred': 193,\n",
       " 'amount': 194,\n",
       " 'seventy': 195,\n",
       " 'thousand': 196,\n",
       " 'pounds': 197,\n",
       " 'friendly': 198,\n",
       " 'am': 199,\n",
       " 'assured': 200,\n",
       " 'nearer': 201,\n",
       " 'termination': 202,\n",
       " 'now': 203,\n",
       " 'than': 204,\n",
       " 'when': 205,\n",
       " 'begun': 206,\n",
       " 'another': 207,\n",
       " 'well-known': 208,\n",
       " 'yet': 209,\n",
       " 'decided': 210,\n",
       " 'close': 211,\n",
       " 'last': 212,\n",
       " 'century': 213,\n",
       " 'more': 214,\n",
       " 'double': 215,\n",
       " 'swallowed': 216,\n",
       " 'up': 217,\n",
       " 'if': 218,\n",
       " 'wanted': 219,\n",
       " 'authorities': 220,\n",
       " 'for': 221,\n",
       " 'jarndyce': 222,\n",
       " 'could': 223,\n",
       " 'rain': 224,\n",
       " 'them': 225,\n",
       " 'shame': 226,\n",
       " 'of—a': 227,\n",
       " 'only': 228,\n",
       " 'offer': 229,\n",
       " 'word': 230,\n",
       " 'remark': 231,\n",
       " 'possibility': 232,\n",
       " 'called': 233,\n",
       " 'spontaneous': 234,\n",
       " 'combustion': 235,\n",
       " 'denied': 236,\n",
       " 'since': 237,\n",
       " 'death': 238,\n",
       " 'krook;': 239,\n",
       " 'good': 240,\n",
       " 'friend': 241,\n",
       " 'lewes': 242,\n",
       " 'quite': 243,\n",
       " 'mistaken': 244,\n",
       " 'soon': 245,\n",
       " 'found': 246,\n",
       " 'supposing': 247,\n",
       " 'thing': 248,\n",
       " 'abandoned': 249,\n",
       " 'all': 250,\n",
       " 'published': 251,\n",
       " 'ingenious': 252,\n",
       " 'letters': 253,\n",
       " 'event': 254,\n",
       " 'chronicled': 255,\n",
       " 'arguing': 256,\n",
       " 'possibly': 257,\n",
       " 'need': 258,\n",
       " 'observe': 259,\n",
       " 'wilfully': 260,\n",
       " 'negligently': 261,\n",
       " 'mislead': 262,\n",
       " 'readers': 263,\n",
       " 'wrote': 264,\n",
       " 'description': 265,\n",
       " 'took': 266,\n",
       " 'pains': 267,\n",
       " 'investigate': 268,\n",
       " 'are': 269,\n",
       " 'about': 270,\n",
       " 'cases': 271,\n",
       " 'record': 272,\n",
       " 'famous': 273,\n",
       " 'countess': 274,\n",
       " 'cornelia': 275,\n",
       " 'de': 276,\n",
       " 'baudi': 277,\n",
       " 'cesenate': 278,\n",
       " 'minutely': 279,\n",
       " 'investigated': 280,\n",
       " 'described': 281,\n",
       " 'giuseppe': 282,\n",
       " 'bianchini': 283,\n",
       " 'prebendary': 284,\n",
       " 'verona': 285,\n",
       " 'otherwise': 286,\n",
       " 'distinguished': 287,\n",
       " 'account': 288,\n",
       " '1731': 289,\n",
       " 'afterwards': 290,\n",
       " 'republished': 291,\n",
       " 'rome': 292,\n",
       " 'appearances': 293,\n",
       " 'beyond': 294,\n",
       " 'rational': 295,\n",
       " 'doubt': 296,\n",
       " 'observed': 297,\n",
       " 'krook’s': 298,\n",
       " 'next': 299,\n",
       " 'instance': 300,\n",
       " 'happened': 301,\n",
       " 'rheims': 302,\n",
       " 'six': 303,\n",
       " 'earlier': 304,\n",
       " 'historian': 305,\n",
       " 'le': 306,\n",
       " 'cat': 307,\n",
       " 'renowned': 308,\n",
       " 'surgeons': 309,\n",
       " 'produced': 310,\n",
       " 'france': 311,\n",
       " 'woman': 312,\n",
       " 'whose': 313,\n",
       " 'husband': 314,\n",
       " 'ignorantly': 315,\n",
       " 'convicted': 316,\n",
       " 'having': 317,\n",
       " 'murdered': 318,\n",
       " 'her;': 319,\n",
       " 'solemn': 320,\n",
       " 'appeal': 321,\n",
       " 'higher': 322,\n",
       " 'acquitted': 323,\n",
       " 'because': 324,\n",
       " 'shown': 325,\n",
       " 'upon': 326,\n",
       " 'evidence': 327,\n",
       " 'she': 328,\n",
       " 'died': 329,\n",
       " 'name': 330,\n",
       " 'given': 331,\n",
       " 'necessary': 332,\n",
       " 'add': 333,\n",
       " 'notable': 334,\n",
       " 'facts': 335,\n",
       " 'general': 336,\n",
       " 'reference': 337,\n",
       " 'page': 338,\n",
       " '30': 339,\n",
       " 'vol': 340,\n",
       " 'ii': 341,\n",
       " '*': 342,\n",
       " 'recorded': 343,\n",
       " 'opinions': 344,\n",
       " 'experiences': 345,\n",
       " 'medical': 346,\n",
       " 'professors': 347,\n",
       " 'french': 348,\n",
       " 'english': 349,\n",
       " 'scotch': 350,\n",
       " 'modern': 351,\n",
       " 'days': 352,\n",
       " 'contenting': 353,\n",
       " 'myself': 354,\n",
       " 'observing': 355,\n",
       " 'shall': 356,\n",
       " 'abandon': 357,\n",
       " 'considerable': 358,\n",
       " 'testimony': 359,\n",
       " 'human': 360,\n",
       " 'occurrences': 361,\n",
       " 'usually': 362,\n",
       " 'received': 363,\n",
       " 'start-of-sentence**': 364,\n",
       " 'bleak': 365,\n",
       " 'house': 366,\n",
       " 'purposely': 367,\n",
       " 'dwelt': 368,\n",
       " 'romantic': 369,\n",
       " 'side': 370,\n",
       " 'familiar': 371,\n",
       " 'things': 372,\n",
       " '*transcriber’s': 373,\n",
       " 'note': 374,\n",
       " 'referred': 375,\n",
       " 'specific': 376,\n",
       " 'printed': 377,\n",
       " 'project': 378,\n",
       " 'gutenberg': 379,\n",
       " 'edition': 380,\n",
       " 'pertinent': 381,\n",
       " 'information': 382,\n",
       " 'chapter': 383,\n",
       " 'xxx': 384,\n",
       " 'paragraph': 385,\n",
       " '90': 386,\n",
       " '**': 387,\n",
       " 'very': 388,\n",
       " 'clearly': 389,\n",
       " 'dentist': 390,\n",
       " 'occurred': 391,\n",
       " 'town': 392,\n",
       " 'columbus': 393,\n",
       " 'united': 394,\n",
       " 'states': 395,\n",
       " 'america': 396,\n",
       " 'recently': 397,\n",
       " 'german': 398,\n",
       " 'kept': 399,\n",
       " 'liquor-shop': 400,\n",
       " 'inveterate': 401,\n",
       " 'drunkard': 402,\n",
       " 'london': 403,\n",
       " 'michaelmas': 404,\n",
       " 'term': 405,\n",
       " 'over': 406,\n",
       " 'lord': 407,\n",
       " 'chancellor': 408,\n",
       " 'sitting': 409,\n",
       " 'lincoln’s': 410,\n",
       " 'inn': 411,\n",
       " 'hall': 412,\n",
       " 'implacable': 413,\n",
       " 'november': 414,\n",
       " 'weather': 415,\n",
       " 'mud': 416,\n",
       " 'streets': 417,\n",
       " 'waters': 418,\n",
       " 'newly': 419,\n",
       " 'retired': 420,\n",
       " 'face': 421,\n",
       " 'earth': 422,\n",
       " 'would': 423,\n",
       " 'wonderful': 424,\n",
       " 'meet': 425,\n",
       " 'megalosaurus': 426,\n",
       " 'feet': 427,\n",
       " 'long': 428,\n",
       " 'waddling': 429,\n",
       " 'elephantine': 430,\n",
       " 'lizard': 431,\n",
       " 'holborn': 432,\n",
       " 'hill': 433,\n",
       " 'smoke': 434,\n",
       " 'lowering': 435,\n",
       " 'down': 436,\n",
       " 'chimney-pots': 437,\n",
       " 'making': 438,\n",
       " 'soft': 439,\n",
       " 'black': 440,\n",
       " 'drizzle': 441,\n",
       " 'flakes': 442,\n",
       " 'soot': 443,\n",
       " 'big': 444,\n",
       " 'full-grown': 445,\n",
       " 'snowflakes—gone': 446,\n",
       " 'into': 447,\n",
       " 'mourning': 448,\n",
       " 'imagine': 449,\n",
       " 'sun': 450,\n",
       " 'dogs': 451,\n",
       " 'undistinguishable': 452,\n",
       " 'mire': 453,\n",
       " 'horses': 454,\n",
       " 'scarcely': 455,\n",
       " 'better;': 456,\n",
       " 'splashed': 457,\n",
       " 'their': 458,\n",
       " 'blinkers': 459,\n",
       " 'foot': 460,\n",
       " 'passengers': 461,\n",
       " 'jostling': 462,\n",
       " 'another’s': 463,\n",
       " 'umbrellas': 464,\n",
       " 'infection': 465,\n",
       " 'ill': 466,\n",
       " 'temper': 467,\n",
       " 'losing': 468,\n",
       " 'foot-hold': 469,\n",
       " 'street-corners': 470,\n",
       " 'where': 471,\n",
       " 'tens': 472,\n",
       " 'thousands': 473,\n",
       " 'slipping': 474,\n",
       " 'sliding': 475,\n",
       " 'day': 476,\n",
       " 'broke': 477,\n",
       " 'ever': 478,\n",
       " 'adding': 479,\n",
       " 'new': 480,\n",
       " 'deposits': 481,\n",
       " 'crust': 482,\n",
       " 'sticking': 483,\n",
       " 'those': 484,\n",
       " 'points': 485,\n",
       " 'tenaciously': 486,\n",
       " 'pavement': 487,\n",
       " 'accumulating': 488,\n",
       " 'compound': 489,\n",
       " 'interest': 490,\n",
       " 'fog': 491,\n",
       " 'everywhere': 492,\n",
       " 'river': 493,\n",
       " 'flows': 494,\n",
       " 'among': 495,\n",
       " 'green': 496,\n",
       " 'aits': 497,\n",
       " 'meadows;': 498,\n",
       " 'rolls': 499,\n",
       " 'defiled': 500,\n",
       " 'tiers': 501,\n",
       " 'shipping': 502,\n",
       " 'waterside': 503,\n",
       " 'pollutions': 504,\n",
       " 'great': 505,\n",
       " 'dirty': 506,\n",
       " 'city': 507,\n",
       " 'essex': 508,\n",
       " 'marshes': 509,\n",
       " 'kentish': 510,\n",
       " 'heights': 511,\n",
       " 'creeping': 512,\n",
       " 'cabooses': 513,\n",
       " 'collier-brigs;': 514,\n",
       " 'lying': 515,\n",
       " 'out': 516,\n",
       " 'yards': 517,\n",
       " 'hovering': 518,\n",
       " 'rigging': 519,\n",
       " 'ships;': 520,\n",
       " 'drooping': 521,\n",
       " 'gunwales': 522,\n",
       " 'barges': 523,\n",
       " 'small': 524,\n",
       " 'boats': 525,\n",
       " 'eyes': 526,\n",
       " 'throats': 527,\n",
       " 'ancient': 528,\n",
       " 'greenwich': 529,\n",
       " 'pensioners': 530,\n",
       " 'wheezing': 531,\n",
       " 'firesides': 532,\n",
       " 'wards;': 533,\n",
       " 'stem': 534,\n",
       " 'bowl': 535,\n",
       " 'afternoon': 536,\n",
       " 'pipe': 537,\n",
       " 'wrathful': 538,\n",
       " 'skipper': 539,\n",
       " 'his': 540,\n",
       " 'cabin;': 541,\n",
       " 'cruelly': 542,\n",
       " 'pinching': 543,\n",
       " 'toes': 544,\n",
       " 'fingers': 545,\n",
       " 'shivering': 546,\n",
       " 'little': 547,\n",
       " '’prentice': 548,\n",
       " 'boy': 549,\n",
       " 'deck': 550,\n",
       " 'chance': 551,\n",
       " 'people': 552,\n",
       " 'bridges': 553,\n",
       " 'peeping': 554,\n",
       " 'parapets': 555,\n",
       " 'nether': 556,\n",
       " 'sky': 557,\n",
       " 'round': 558,\n",
       " 'they': 559,\n",
       " 'balloon': 560,\n",
       " 'hanging': 561,\n",
       " 'misty': 562,\n",
       " 'clouds': 563,\n",
       " 'gas': 564,\n",
       " 'looming': 565,\n",
       " 'through': 566,\n",
       " 'divers': 567,\n",
       " 'places': 568,\n",
       " 'may': 569,\n",
       " 'spongey': 570,\n",
       " 'fields': 571,\n",
       " 'seen': 572,\n",
       " 'loom': 573,\n",
       " 'husbandman': 574,\n",
       " 'ploughboy': 575,\n",
       " 'shops': 576,\n",
       " 'lighted': 577,\n",
       " 'two': 578,\n",
       " 'hours': 579,\n",
       " 'time—as': 580,\n",
       " 'seems': 581,\n",
       " 'haggard': 582,\n",
       " 'unwilling': 583,\n",
       " 'look': 584,\n",
       " 'raw': 585,\n",
       " 'rawest': 586,\n",
       " 'dense': 587,\n",
       " 'densest': 588,\n",
       " 'muddy': 589,\n",
       " 'muddiest': 590,\n",
       " 'near': 591,\n",
       " 'leaden-headed': 592,\n",
       " 'old': 593,\n",
       " 'obstruction': 594,\n",
       " 'appropriate': 595,\n",
       " 'ornament': 596,\n",
       " 'threshold': 597,\n",
       " 'corporation': 598,\n",
       " 'temple': 599,\n",
       " 'bar': 600,\n",
       " 'hard': 601,\n",
       " 'heart': 602,\n",
       " 'sits': 603,\n",
       " 'high': 604,\n",
       " 'never': 605,\n",
       " 'can': 606,\n",
       " 'come': 607,\n",
       " 'thick': 608,\n",
       " 'deep': 609,\n",
       " 'assort': 610,\n",
       " 'groping': 611,\n",
       " 'floundering': 612,\n",
       " 'condition': 613,\n",
       " 'pestilent': 614,\n",
       " 'hoary': 615,\n",
       " 'sinners': 616,\n",
       " 'holds': 617,\n",
       " 'sight': 618,\n",
       " 'heaven': 619,\n",
       " 'ought': 620,\n",
       " 'here—as': 621,\n",
       " 'is—with': 622,\n",
       " 'foggy': 623,\n",
       " 'glory': 624,\n",
       " 'head': 625,\n",
       " 'softly': 626,\n",
       " 'fenced': 627,\n",
       " 'crimson': 628,\n",
       " 'cloth': 629,\n",
       " 'curtains': 630,\n",
       " 'addressed': 631,\n",
       " 'large': 632,\n",
       " 'advocate': 633,\n",
       " 'whiskers': 634,\n",
       " 'voice': 635,\n",
       " 'interminable': 636,\n",
       " 'brief': 637,\n",
       " 'outwardly': 638,\n",
       " 'directing': 639,\n",
       " 'contemplation': 640,\n",
       " 'lantern': 641,\n",
       " 'roof': 642,\n",
       " 'see': 643,\n",
       " 'nothing': 644,\n",
       " 'score': 645,\n",
       " 'members': 646,\n",
       " 'be—as': 647,\n",
       " 'are—mistily': 648,\n",
       " 'engaged': 649,\n",
       " 'ten': 650,\n",
       " 'stages': 651,\n",
       " 'endless': 652,\n",
       " 'cause': 653,\n",
       " 'tripping': 654,\n",
       " 'slippery': 655,\n",
       " 'precedents': 656,\n",
       " 'knee-deep': 657,\n",
       " 'technicalities': 658,\n",
       " 'running': 659,\n",
       " 'goat-hair': 660,\n",
       " 'horsehair': 661,\n",
       " 'warded': 662,\n",
       " 'heads': 663,\n",
       " 'against': 664,\n",
       " 'walls': 665,\n",
       " 'words': 666,\n",
       " 'pretence': 667,\n",
       " 'equity': 668,\n",
       " 'serious': 669,\n",
       " 'faces': 670,\n",
       " 'players': 671,\n",
       " 'various': 672,\n",
       " 'solicitors': 673,\n",
       " 'three': 674,\n",
       " 'inherited': 675,\n",
       " 'fathers': 676,\n",
       " 'fortune': 677,\n",
       " 'not?—ranged': 678,\n",
       " 'line': 679,\n",
       " 'matted': 680,\n",
       " 'you': 681,\n",
       " 'vain': 682,\n",
       " 'bottom': 683,\n",
       " 'between': 684,\n",
       " 'registrar’s': 685,\n",
       " 'red': 686,\n",
       " 'table': 687,\n",
       " 'silk': 688,\n",
       " 'gowns': 689,\n",
       " 'bills': 690,\n",
       " 'cross-bills': 691,\n",
       " 'answers': 692,\n",
       " 'rejoinders': 693,\n",
       " 'injunctions': 694,\n",
       " 'affidavits': 695,\n",
       " 'issues': 696,\n",
       " 'references': 697,\n",
       " 'masters': 698,\n",
       " 'masters’': 699,\n",
       " 'reports': 700,\n",
       " 'mountains': 701,\n",
       " 'costly': 702,\n",
       " 'nonsense': 703,\n",
       " 'piled': 704,\n",
       " 'dim': 705,\n",
       " 'wasting': 706,\n",
       " 'candles': 707,\n",
       " 'there;': 708,\n",
       " 'hang': 709,\n",
       " 'heavy': 710,\n",
       " 'get': 711,\n",
       " 'out;': 712,\n",
       " 'stained-glass': 713,\n",
       " 'windows': 714,\n",
       " 'lose': 715,\n",
       " 'colour': 716,\n",
       " 'admit': 717,\n",
       " 'light': 718,\n",
       " 'place;': 719,\n",
       " 'uninitiated': 720,\n",
       " 'peep': 721,\n",
       " 'glass': 722,\n",
       " 'panes': 723,\n",
       " 'door': 724,\n",
       " 'deterred': 725,\n",
       " 'entrance': 726,\n",
       " 'owlish': 727,\n",
       " 'aspect': 728,\n",
       " 'drawl': 729,\n",
       " 'languidly': 730,\n",
       " 'echoing': 731,\n",
       " 'padded': 732,\n",
       " 'dais': 733,\n",
       " 'looks': 734,\n",
       " 'attendant': 735,\n",
       " 'wigs': 736,\n",
       " 'stuck': 737,\n",
       " 'fog-bank!': 738,\n",
       " 'decaying': 739,\n",
       " 'houses': 740,\n",
       " 'blighted': 741,\n",
       " 'lands': 742,\n",
       " 'every': 743,\n",
       " 'shire': 744,\n",
       " 'worn-out': 745,\n",
       " 'lunatic': 746,\n",
       " 'madhouse': 747,\n",
       " 'dead': 748,\n",
       " 'churchyard': 749,\n",
       " 'ruined': 750,\n",
       " 'suitor': 751,\n",
       " 'slipshod': 752,\n",
       " 'heels': 753,\n",
       " 'threadbare': 754,\n",
       " 'dress': 755,\n",
       " 'borrowing': 756,\n",
       " 'begging': 757,\n",
       " 'man’s': 758,\n",
       " 'acquaintance': 759,\n",
       " 'gives': 760,\n",
       " 'monied': 761,\n",
       " 'abundantly': 762,\n",
       " 'wearying': 763,\n",
       " 'right': 764,\n",
       " 'exhausts': 765,\n",
       " 'finances': 766,\n",
       " 'patience': 767,\n",
       " 'courage': 768,\n",
       " 'hope': 769,\n",
       " 'overthrows': 770,\n",
       " 'brain': 771,\n",
       " 'breaks': 772,\n",
       " 'honourable': 773,\n",
       " 'man': 774,\n",
       " 'practitioners': 775,\n",
       " 'give—who': 776,\n",
       " 'does': 777,\n",
       " 'often': 778,\n",
       " 'give—the': 779,\n",
       " 'warning': 780,\n",
       " '“suffer': 781,\n",
       " 'done': 782,\n",
       " 'rather': 783,\n",
       " 'here!”': 784,\n",
       " 'happen': 785,\n",
       " 'chancellor’s': 786,\n",
       " 'murky': 787,\n",
       " 'besides': 788,\n",
       " 'mentioned?': 789,\n",
       " 'registrar': 790,\n",
       " 'below': 791,\n",
       " 'wig': 792,\n",
       " 'gown;': 793,\n",
       " 'maces': 794,\n",
       " 'petty-bags': 795,\n",
       " 'privy': 796,\n",
       " 'purses': 797,\n",
       " 'whatever': 798,\n",
       " 'legal': 799,\n",
       " 'suits': 800,\n",
       " 'yawning': 801,\n",
       " 'crumb': 802,\n",
       " 'amusement': 803,\n",
       " 'falls': 804,\n",
       " 'hand': 805,\n",
       " 'squeezed': 806,\n",
       " 'dry': 807,\n",
       " 'short-hand': 808,\n",
       " 'writers': 809,\n",
       " 'reporters': 810,\n",
       " 'newspapers': 811,\n",
       " 'invariably': 812,\n",
       " 'decamp': 813,\n",
       " 'rest': 814,\n",
       " 'regulars': 815,\n",
       " 'comes': 816,\n",
       " 'blank': 817,\n",
       " 'standing': 818,\n",
       " 'seat': 819,\n",
       " 'better': 820,\n",
       " 'peer': 821,\n",
       " 'curtained': 822,\n",
       " 'sanctuary': 823,\n",
       " 'mad': 824,\n",
       " 'bonnet': 825,\n",
       " 'always': 826,\n",
       " 'rising': 827,\n",
       " 'expecting': 828,\n",
       " 'incomprehensible': 829,\n",
       " 'judgment': 830,\n",
       " 'her': 831,\n",
       " 'favour': 832,\n",
       " 'say': 833,\n",
       " 'really': 834,\n",
       " 'party': 835,\n",
       " 'knows': 836,\n",
       " 'certain': 837,\n",
       " 'cares': 838,\n",
       " 'carries': 839,\n",
       " 'litter': 840,\n",
       " 'reticule': 841,\n",
       " 'calls': 842,\n",
       " 'documents': 843,\n",
       " 'principally': 844,\n",
       " 'consisting': 845,\n",
       " 'paper': 846,\n",
       " 'matches': 847,\n",
       " 'lavender': 848,\n",
       " 'sallow': 849,\n",
       " 'prisoner': 850,\n",
       " 'custody': 851,\n",
       " 'half-dozenth': 852,\n",
       " 'make': 853,\n",
       " 'personal': 854,\n",
       " 'application': 855,\n",
       " '“to': 856,\n",
       " 'purge': 857,\n",
       " 'himself': 858,\n",
       " 'contempt': 859,\n",
       " 'being': 860,\n",
       " 'solitary': 861,\n",
       " 'surviving': 862,\n",
       " 'executor': 863,\n",
       " 'fallen': 864,\n",
       " 'state': 865,\n",
       " 'conglomeration': 866,\n",
       " 'accounts': 867,\n",
       " 'pretended': 868,\n",
       " 'knowledge': 869,\n",
       " 'likely': 870,\n",
       " 'meantime': 871,\n",
       " 'prospects': 872,\n",
       " 'life': 873,\n",
       " 'ended': 874,\n",
       " 'periodically': 875,\n",
       " 'appears': 876,\n",
       " 'shropshire': 877,\n",
       " 'efforts': 878,\n",
       " 'address': 879,\n",
       " 'day’s': 880,\n",
       " 'business': 881,\n",
       " 'understand': 882,\n",
       " 'legally': 883,\n",
       " 'ignorant': 884,\n",
       " 'existence': 885,\n",
       " 'after': 886,\n",
       " 'desolate': 887,\n",
       " 'quarter': 888,\n",
       " 'plants': 889,\n",
       " 'place': 890,\n",
       " 'keeps': 891,\n",
       " 'ready': 892,\n",
       " 'call': 893,\n",
       " 'lord!”': 894,\n",
       " 'sonorous': 895,\n",
       " 'complaint': 896,\n",
       " 'instant': 897,\n",
       " 'few': 898,\n",
       " 'lawyers’': 899,\n",
       " 'clerks': 900,\n",
       " 'others': 901,\n",
       " 'linger': 902,\n",
       " 'furnishing': 903,\n",
       " 'fun': 904,\n",
       " 'enlivening': 905,\n",
       " 'dismal': 906,\n",
       " 'drones': 907,\n",
       " 'scarecrow': 908,\n",
       " 'course': 909,\n",
       " 'become': 910,\n",
       " 'complicated': 911,\n",
       " 'alive': 912,\n",
       " 'parties': 913,\n",
       " 'least': 914,\n",
       " 'lawyers': 915,\n",
       " 'talk': 916,\n",
       " 'five': 917,\n",
       " 'minutes': 918,\n",
       " 'without': 919,\n",
       " 'coming': 920,\n",
       " 'total': 921,\n",
       " 'disagreement': 922,\n",
       " 'premises': 923,\n",
       " 'innumerable': 924,\n",
       " 'children': 925,\n",
       " 'born': 926,\n",
       " 'cause;': 927,\n",
       " 'young': 928,\n",
       " 'married': 929,\n",
       " 'it;': 930,\n",
       " 'scores': 931,\n",
       " 'persons': 932,\n",
       " 'deliriously': 933,\n",
       " 'themselves': 934,\n",
       " 'knowing': 935,\n",
       " 'how': 936,\n",
       " 'why;': 937,\n",
       " 'families': 938,\n",
       " 'legendary': 939,\n",
       " 'hatreds': 940,\n",
       " 'plaintiff': 941,\n",
       " 'defendant': 942,\n",
       " 'promised': 943,\n",
       " 'rocking-horse': 944,\n",
       " 'settled': 945,\n",
       " 'grown': 946,\n",
       " 'possessed': 947,\n",
       " 'real': 948,\n",
       " 'horse': 949,\n",
       " 'trotted': 950,\n",
       " 'away': 951,\n",
       " 'world': 952,\n",
       " 'fair': 953,\n",
       " 'wards': 954,\n",
       " 'faded': 955,\n",
       " 'mothers': 956,\n",
       " 'grandmothers;': 957,\n",
       " 'procession': 958,\n",
       " 'chancellors': 959,\n",
       " 'gone': 960,\n",
       " 'legion': 961,\n",
       " 'transformed': 962,\n",
       " 'mere': 963,\n",
       " 'mortality;': 964,\n",
       " 'jarndyces': 965,\n",
       " 'left': 966,\n",
       " 'perhaps': 967,\n",
       " 'tom': 968,\n",
       " 'despair': 969,\n",
       " 'blew': 970,\n",
       " 'brains': 971,\n",
       " 'coffee-house': 972,\n",
       " 'lane;': 973,\n",
       " 'drags': 974,\n",
       " 'dreary': 975,\n",
       " 'length': 976,\n",
       " 'perennially': 977,\n",
       " 'hopeless': 978,\n",
       " 'passed': 979,\n",
       " 'many': 980,\n",
       " 'profession': 981,\n",
       " 'master': 982,\n",
       " '“in': 983,\n",
       " 'somebody': 984,\n",
       " 'said': 985,\n",
       " 'blue-nosed': 986,\n",
       " 'bulbous-shoed': 987,\n",
       " 'benchers': 988,\n",
       " 'select': 989,\n",
       " 'port-wine': 990,\n",
       " 'committee': 991,\n",
       " 'dinner': 992,\n",
       " 'articled': 993,\n",
       " 'habit': 994,\n",
       " 'fleshing': 995,\n",
       " 'wit': 996,\n",
       " 'handled': 997,\n",
       " 'neatly': 998,\n",
       " 'correcting': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "myVocab.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78c2db28-99c0-4569-9e19-102d90daf692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "#vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "#vocab.set_default_index(vocab[\"<unk>\"])\n",
    "#vocab(tokens[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "840ca675-54f9-48f1-b915-54e22bf0abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_map = myVocab.tokens #vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d1e4407-b80a-42a5-bd7b-589576be6d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start-of-sentence is a new dress a new custom a new singer a new dancer a new form of jewellery a new dwarf or giant a new chapel a new anything to be set up? there are deferential people in a dozen callings whom my lady dedlock suspects of nothing but prostration before her who can tell you how to manage her as if she were a baby who do nothing but nurse her all their lives who humbly affecting to follow with profound subservience lead her and her whole troop after them; who in hooking one hook all and bear them off as lemuel gulliver bore away the stately fleet of the majestic lilliput end-of-sentence'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splited_data[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49171cbb-4dd0-4b78-9a51-e4f5cd9e2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "\n",
    "for sentence in splited_data:\n",
    "    data=sentence.split()\n",
    "    for i in range(1,len(data)):\n",
    "        X=data[0:i]\n",
    "        y = data[i]\n",
    "        training_data.append((X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f718ebe-62c5-4dae-a6ac-9e51be13004b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76182"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00b9f1ca-3f95-4258-aeaa-c4f05a0fd6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['start-of-sentence',\n",
       "   'preface',\n",
       "   'a',\n",
       "   'chancery',\n",
       "   'judge',\n",
       "   'once',\n",
       "   'had',\n",
       "   'the',\n",
       "   'kindness',\n",
       "   'to',\n",
       "   'inform',\n",
       "   'me',\n",
       "   'as',\n",
       "   'one',\n",
       "   'of',\n",
       "   'a',\n",
       "   'company',\n",
       "   'of',\n",
       "   'some',\n",
       "   'hundred',\n",
       "   'and'],\n",
       "  'fifty')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[20:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0cfa6e9c-dcd8-4916-bcc7-659c79fa5d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92663eb6-c080-4904-8510-a23d37ec34b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second method\n",
    "\n",
    "training_data_cut = []\n",
    "\n",
    "for sentence in splited_data:\n",
    "    data=sentence.split()\n",
    "    for i in range(1,len(data)):\n",
    "        X=data[max([0,i-WINDOW]):i]\n",
    "        y = data[i]\n",
    "        training_data_cut.append((X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d06bf5fe-a942-4d7b-9e4e-1ef40e8ee3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['me', 'as', 'one', 'of', 'a', 'company', 'of', 'some', 'hundred', 'and'],\n",
       "  'fifty')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_cut[20:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5c122cd-298a-453e-8894-b1e2f7991b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_token = token_map\n",
    "\n",
    "def word_to_token_wrapper(word: str) -> int:\n",
    "    if word not in word_to_token:\n",
    "        return word_to_token[\"<unk>\"]\n",
    "    return word_to_token[word]\n",
    "\n",
    "def list_sentence_to_token(sentence_list)->int:\n",
    "    return [word_to_token_wrapper(word) for word in sentence_list]\n",
    "\n",
    "def sentence_to_token(sentence):\n",
    "    return [word_to_token_wrapper(word) for word in sentence.split()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4387afe-d8bc-4148-b468-15aa3b2ee7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[440]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_token(\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f290f3b2-10c5-404c-a0f4-e9614fc3c6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['start-of-sentence', 'preface', 'a', 'chancery', 'judge'], 'once')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_cut[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "244187ee-b1c8-4b44-83d7-dd8c518662c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_tokenized = [ (list_sentence_to_token(a),word_to_token_wrapper(b) ) for a,b in training_data_cut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2c6c21c-4854-4e29-b413-f390e3653532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([73, 74, 75, 44, 7, 76, 77, 78, 79, 80], 81),\n",
       " ([74, 75, 44, 7, 76, 77, 78, 79, 80, 81], 82),\n",
       " ([75, 44, 7, 76, 77, 78, 79, 80, 81, 82], 83),\n",
       " ([44, 7, 76, 77, 78, 79, 80, 81, 82, 83], 7),\n",
       " ([7, 76, 77, 78, 79, 80, 81, 82, 83, 7], 84)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_tokenized[110:115]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d820d50d-7e6b-4ab1-8385-795a0694b913",
   "metadata": {},
   "source": [
    "## training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74579798-7a23-456b-8680-6921454d61e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SentenceCompletionDataset(Dataset):\n",
    "    def __init__(self, data, word_to_index):\n",
    "        self.data = data\n",
    "        self.word_to_index = word_to_index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_sequence, target_word = self.data[idx]\n",
    "        input_indices = [self.word_to_index[word] for word in input_sequence]\n",
    "        target_index = self.word_to_index[target_word]\n",
    "        return torch.tensor(input_indices, dtype=torch.long), torch.tensor(target_index, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e4bfb11-3185-4466-9c2a-3d1c7171b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming train_data is a list of sentences\n",
    "train_sentences, val_sentences = train_test_split(training_data_tokenized, test_size=0.5, random_state=42)\n",
    "\n",
    "# Now, apply the masking and preparation logic to both train_sentences and val_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef9f8b5-2e16-4feb-9747-fa2f40f18a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "38d86824-d260-49d7-8963-4dd9afd0cb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens, target = self.data[idx]\n",
    "        return torch.tensor(tokens, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "# Assuming `data` is your list of mappings\n",
    "dataset = TokenDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=60, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3675f709-5751-4b03-9046-4ed8aedca1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tokens, targets = zip(*batch)\n",
    "    # Pad the sequences to the maximum length in the batch\n",
    "    tokens_padded = pad_sequence([torch.tensor(seq) for seq in tokens], batch_first=True, padding_value=0)\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "    return tokens_padded, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "00a5f36f-3a23-456c-91e0-be7e2e70d5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38091"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eb18544b-134f-458f-a51e-03ebb168c25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9233"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(word_to_token)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42158537-22cc-402d-8401-3ffa811693d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,dropout_rate=0.5):\n",
    "        super(SimpleLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Add dropout layer\n",
    "\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        embedded = self.embedding(tokens)  # [batch_size, seq_len, embedding_dim]\n",
    "        # Aggregate embeddings, e.g., by averaging\n",
    "        aggregated = embedded.mean(dim=1)  # [batch_size, embedding_dim]\n",
    "        hidden = self.relu(self.linear1(aggregated))  # [batch_size, hidden_dim]\n",
    "        output = self.linear2(hidden)  # [batch_size, output_dim]\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "\"\"\"\n",
    "embedding_dim = 60\n",
    "hidden_dim = 30\n",
    "\"\"\"\n",
    "embedding_dim = 60\n",
    "hidden_dim = 30\n",
    "output_dim = vocab_size  # Same as vocab size for prediction\n",
    "\n",
    "model = SimpleLM(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e48738-f9ae-44fd-b506-5adc25eab787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "25a63ff8-a177-479e-8eac-3173359c62a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38091"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "762211de-4b69-46c6-9491-35d0f19cc23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5d083c5d-019f-45bf-bd81-a4e42b1db816",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TokenDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=5, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "57b5517e-15b1-4ae5-8305-e66a669d5298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/bv3z08h91lggkmhcp2n7yzh80000gn/T/ipykernel_77890/763346120.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens_padded = pad_sequence([torch.tensor(seq) for seq in tokens], batch_first=True, padding_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.953255592235541\n",
      "Epoch 11, Loss: 4.861751239652442\n",
      "Epoch 21, Loss: 4.175530559920563\n",
      "Epoch 31, Loss: 3.8444812300324456\n",
      "Epoch 41, Loss: 3.644073111845261\n",
      "Epoch 51, Loss: 3.5131063884614817\n",
      "Epoch 61, Loss: 3.4258032961926883\n",
      "Epoch 71, Loss: 3.3583802961567004\n",
      "Epoch 81, Loss: 3.308972013420689\n",
      "Epoch 91, Loss: 3.2768861328871766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nembedding_dim = 50\\nhidden_dim = 20\\ndropout = 0.5\\nNew token technique : Loss: 0.33178764806329597\\n\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 100  # Or whatever suits your dataset\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for tokens, target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(tokens)\n",
    "        loss = criterion(output, target.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    if epoch %10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}')\n",
    "\n",
    "# batch 5 => Epoch 141, Loss: 0.3472299749050117\n",
    "# batch 7 => Epoch 141, Loss: 0.3254897424296818\n",
    "\n",
    "# batch 5 and window 5 => Epoch 141, Loss: 0.2996615952255913\n",
    "\"\"\"\n",
    "embedding_dim = 80\n",
    "hidden_dim = 50\n",
    "0.26723970297246136\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "embedding_dim = 40\n",
    "hidden_dim = 20\n",
    "0.37902324068592874\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "embedding_dim = 50\n",
    "hidden_dim = 20\n",
    "dropout = 0.5\n",
    "0.3662537602579914\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "embedding_dim = 50\n",
    "hidden_dim = 20\n",
    "dropout = 0.5\n",
    "New token technique : Loss: 0.33178764806329597\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e4f7d5eb-e01d-48a3-adac-63729f150b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([0, 2236, 12, 45, 2034, 14, 45], 2237),\n",
       " ([0, 205, 39, 199, 516, 14, 3124, 39, 607], 18),\n",
       " ([7, 4625, 14, 7, 593, 2343, 9, 4626, 119, 4627], 4628),\n",
       " ([250, 63, 191, 22, 228, 310, 1207, 3116, 36, 25], 8406),\n",
       " ([58, 1829, 12, 51, 127, 5942, 57, 238, 44, 7], 1456)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ae3f27dd-d8d2-492e-809b-41ff97d8f5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 440]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = sentence_to_token(\"the black\")\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fb477f87-e39e-41b7-bea2-3b8188c90d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_word = {token: word for word, token in word_to_token.items()}\n",
    "\n",
    "def token_to_sentence(tokens):\n",
    "    return \" \".join([token_to_word[t] for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "371788de-aa99-40f3-a1bf-fafcfaee1dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a preface with essential at'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_sentence([2, 1, 110, 160, 36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8a26bc6e-8d86-4539-8689-3a25bad736bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2,   1, 110, 160,  36]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = [2, 1, 110, 160, 36]\n",
    "sequence_tensor = torch.tensor(sequence).unsqueeze(0) \n",
    "sequence_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2acdbf9e-ce24-45d5-bdfe-10897409594f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    output = model(sequence_tensor)\n",
    "    predicted_token_id = output.argmax(dim=1).item()  # Get the index of the max log-probability\n",
    "predicted_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "86d5e762-3274-41d3-9afd-da2848e8e499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'or'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_word[predicted_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e10f12d1-11c7-463b-94af-9bd981e2cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPK = 3\n",
    "\n",
    "def predict_missing_word(sentence=\"black hole is the biggest object in the\"):\n",
    "    last_K = \" \".join(sentence.split()[-WINDOW:])\n",
    "    #print(last_K)                  \n",
    "    sequence = sentence_to_token(last_K)\n",
    "    sequence_tensor = torch.tensor(sequence).unsqueeze(0)  # Add batch dimension\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        output = model(sequence_tensor)\n",
    "        predicted_token_id = output.argmax(dim=1).item()  # Get the index of the max log-probability\n",
    "        _, top5_indices = torch.topk(output[0], TOPK)\n",
    "        predicted_token_id = random.choice(top5_indices.tolist())\n",
    "\n",
    "    # Assuming you have a token_to_word dictionary\n",
    "    token_to_word = {token: word for word, token in word_to_token.items()}\n",
    "    predicted_word = token_to_word[predicted_token_id]\n",
    "    return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "18f15e78-739b-47c5-81c5-17b24d7e3d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_missing_token(tokens):\n",
    "               \n",
    "    sequence = tokens[-WINDOW:]\n",
    "    sequence_tensor = torch.tensor(sequence).unsqueeze(0)  # Add batch dimension\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        output = model(sequence_tensor)\n",
    "        predicted_token_id = output.argmax(dim=1).item()  # Get the index of the max log-probability\n",
    "        \n",
    "        #_, top5_indices = torch.topk(output[0], TOPK)\n",
    "        #predicted_token_id = random.choice(top5_indices.tolist())\n",
    "\n",
    "    return predicted_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5ebe4526-c249-4ba4-bfec-7a25f26fdb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(start_word = \"start-of-sentence\"):\n",
    "    next_word = start_word\n",
    "    #next_word = \"einstein\"\n",
    "    words = []\n",
    "    sentence = next_word\n",
    "    words.append(next_word)\n",
    "    limit = 0\n",
    "    while next_word != \"end-of-sentence\" and limit<20:\n",
    "        #print(\"input \",sentence)\n",
    "        next_word = predict_missing_word(sentence)\n",
    "        #print(\"output \",next_word)\n",
    "        words.append(next_word)\n",
    "        sentence = \" \".join(words)\n",
    "        limit += 1\n",
    "    \n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "13a4aa36-370f-4fb6-90f4-946391f1bfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start-of-sentence is no to child worse to to but never sit to anything tell it know else no much end-of-sentence\n",
      "start-of-sentence the small of guardians you suppose seen themselves cousin beg again the said ada mr classicality classicality classicality classicality end-of-sentence\n",
      "start-of-sentence the floor and back away either pale and the clear the things face head end-of-sentence\n",
      "start-of-sentence is no us in station the court long grey great casts myself end-of-sentence\n",
      "start-of-sentence a decaying her opportunity and and hold and did towards order a knew knew appearance to of the of of\n",
      "start-of-sentence the floor of and peeps parisians law convivial admiration expense quiet to can’t to and stuff had he to me\n",
      "start-of-sentence is no end-of-sentence\n",
      "start-of-sentence a consummate but low country else all about about about no us to to end-of-sentence\n",
      "start-of-sentence is no us end-of-sentence\n",
      "start-of-sentence a consummate of low its hand head inspiring slangular teeth lemon-juice and if her settles he to up? end-of-sentence\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(generate_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa18b764-dc7c-4656-b225-e8637dbda473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef75fc24-8621-4593-973a-ea8afa080304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6b74c871-8701-4916-8b8d-bc4a2b28257f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1287 38091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nembedding_dim = 80\\nhidden_dim = 50\\n259 3462\\n'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "for s in val_sentences:\n",
    "    #print(s)\n",
    "    predicted_token = predict_missing_token(s[0]) \n",
    "    #print(\"predicted token = \", predicted_token)\n",
    "    answer = predicted_token == s[1]\n",
    "\n",
    "    if answer:\n",
    "        correct += answer\n",
    "\n",
    "print()\n",
    "print(correct,len(val_sentences))\n",
    "\"\"\"\n",
    "embedding_dim = 80\n",
    "hidden_dim = 50\n",
    "259 3462\n",
    "\"\"\"\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cd128c04-0712-4795-a2bc-1530b18713ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7143 38091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nembedding_dim = 80\\nhidden_dim = 50\\n7261 8076\\n'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "for s in train_sentences:\n",
    "    #print(s)\n",
    "    predicted_token = predict_missing_token(s[0]) \n",
    "    #print(\"predicted token = \", predicted_token)\n",
    "    answer = predicted_token == s[1]\n",
    "\n",
    "    if answer:\n",
    "        correct += answer\n",
    "\n",
    "print()\n",
    "print(correct,len(train_sentences))\n",
    "\"\"\"\n",
    "embedding_dim = 80\n",
    "hidden_dim = 50\n",
    "7261 8076\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9773e1-a921-45ec-891b-d1c73f2d41a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85264cc-4bba-490b-a392-fc4bd36dc4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d33713-bfaa-42a8-88ad-736d325fd2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf1a33-6dcb-48a7-8f9c-15cdcf0d435c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
