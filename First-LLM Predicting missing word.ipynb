{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f5c693d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start-of-sentence', 'object', 'that', 'has', 'a', 'no-return', 'boundary', 'for', 'other', 'uses', 'see', 'black', 'hole', '(disambiguation)', 'end-of-sentence', 'start-of-sentence', 'direct', 'radio', 'image', 'of', 'a', 'supermassive', 'black', 'hole', 'at', 'the', 'core', 'of', 'messier', '87', 'animated', 'simulation', 'of', 'a', 'schwarzschild', 'black', 'hole', 'with', 'a', 'galaxy', 'passing', 'behind', 'end-of-sentence', 'start-of-sentence', 'around', 'the', 'time', 'of', 'alignment', 'extreme', 'gravitational', 'lensing', 'of', 'the', 'galaxy', 'is', 'observed', 'end-of-sentence', 'start-of-sentence', 'a', 'black', 'hole', 'is', 'a', 'region', 'of', 'spacetime', 'where', 'gravity', 'is', 'so', 'strong', 'that', 'nothing', 'including', 'light', 'and', 'other', 'electromagnetic', 'waves', 'has', 'enough', 'energy', 'to', 'escape', 'it', 'end-of-sentence', 'start-of-sentence', 'einstein', 's', 'theory', 'of', 'general', 'relativity', 'predicts', 'that', 'a', 'sufficiently', 'compact', 'mass']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "import random\n",
    "import re\n",
    "\n",
    "file_path = '/Users/jaafarbendriss/First-LLM/dataset/black_hole_wiki.txt'\n",
    "\n",
    "# Open the file and read its content\n",
    "with open(file_path, 'r') as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "raw_text = file_content\n",
    "\n",
    "def add_start_end_sentence(text):\n",
    "    return \"start-of-sentence \" + text.replace(\".\",\" end-of-sentence start-of-sentence\")\n",
    "\n",
    "\n",
    "def remove_citations(text):\n",
    "    pattern = r\"\\[\\d+\\]\"\n",
    "    cleaned_text = re.sub(pattern, \"\", text)    \n",
    "    return cleaned_text\n",
    "\n",
    "def remove_spaces(text):\n",
    "    pattern = r\"\\s+\"\n",
    "    cleaned_text = re.sub(pattern, \" \", text)    \n",
    "    return cleaned_text\n",
    "\n",
    "def preprocess(text: str):\n",
    "    return remove_spaces(remove_citations(add_start_end_sentence(text).lower().replace(\" s \",\"\").replace(\"\\\\\",\"\").replace(\"'\",\" \").replace('\"',' ').replace(\",\",\" \").replace(\"\\ s\",\" \").replace(\"\\n\",\" \")))\n",
    "\n",
    "preprocessed_text = preprocess(raw_text)\n",
    "\n",
    "Counter(preprocessed_text.split(\" \")).most_common(20)\n",
    "\n",
    "preprocessed_text_array = preprocessed_text.split(\" \")\n",
    "\n",
    "\n",
    "print(preprocessed_text_array[0:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e29b1ff9-43b4-461b-a2bc-7e6fb6bde00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7b4a23b6-9c15-490b-ac86-777a1e396cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "5ee7927f-fad7-4cda-892a-0cce2299a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text = preprocessed_text.replace(\"end-of-sentence start-of-sentence\", \"end-of-sentence69696969start-of-sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "8a82ceb6-22a8-43ad-8fdf-72094a3a7ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "splited_data = preprocessed_text.split(\"69696969\")\n",
    "\n",
    "train_data =  [(i,sentence) for i, sentence in zip( range(0,len(splited_data)), splited_data  )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "dd144cc7-19e8-4594-bf6f-df07623851bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  'start-of-sentence object that has a no-return boundary for other uses see black hole (disambiguation) end-of-sentence'),\n",
       " (1,\n",
       "  'start-of-sentence direct radio image of a supermassive black hole at the core of messier 87 animated simulation of a schwarzschild black hole with a galaxy passing behind end-of-sentence'),\n",
       " (2,\n",
       "  'start-of-sentence around the time of alignment extreme gravitational lensing of the galaxy is observed end-of-sentence'),\n",
       " (3,\n",
       "  'start-of-sentence a black hole is a region of spacetime where gravity is so strong that nothing including light and other electromagnetic waves has enough energy to escape it end-of-sentence'),\n",
       " (4,\n",
       "  'start-of-sentence einstein s theory of general relativity predicts that a sufficiently compact mass can deform spacetime to form a black hole end-of-sentence')]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e0bd9e84-40ab-4fac-8d44-3a17f5f4bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6e6a42db-609e-4ef2-9d9c-396ff1469c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " 'start-of-sentence object that has a no-return boundary for other uses see black hole (disambiguation) end-of-sentence')"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "153faa50-21a4-4c31-b7bf-b9cb41a3887c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'start-of-sentence direct radio image of a supermassive black hole at the core of messier 87 animated simulation of a schwarzschild black hole with a galaxy passing behind end-of-sentence')"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f97c6b0-1669-4702-9d22-dd7bf585c95a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e23f48ff-3888-46e2-a561-15e5a393c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "6ee800a8-b098-4e6f-b548-df70038ba102",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "4724d80b-41a8-4aab-85e4-c3824b4a8c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab()"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1277b59c-4f8a-4f83-84bb-f5d17acd2552",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 37, 10, 66, 6, 0, 186, 16, 68, 0]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(tokens[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "605e2a74-c735-4d23-bd3b-49da62ef6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_map = vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c20f12d7-0b43-48cc-a1cd-f8c0ceef6b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_map[\"black\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "2d399975-a192-4472-8782-f4c7c6fff097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_map[\"hole\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b4b27eca-352a-4943-808c-58156272f143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_map[\"not\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3520fd22-4f2e-4dfe-b790-34493fd87155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def mask_token_in_sentence(sentence, word_to_token):\n",
    "    \"\"\"\n",
    "    Masks a random token in a given sentence and returns the masked sentence and the masked token.\n",
    "    \n",
    "    Args:\n",
    "    - sentence (str): The sentence from which to mask a token.\n",
    "    - word_to_token (dict): A dictionary mapping words to their respective tokens.\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of (masked_sentence, masked_token)\n",
    "    \"\"\"\n",
    "    words = sentence.split()  # Split sentence into words\n",
    "    mask_index = random.randint(0, len(words) - 1)  # Randomly select a word to mask\n",
    "    masked_word = words[mask_index]  # Get the word to mask\n",
    "    masked_token = word_to_token.get(masked_word, None)  # Get the token for the masked word, if available\n",
    "    masked_token = masked_word\n",
    "    \n",
    "    if masked_token is None:\n",
    "        return None, None  # or handle unknown words differently\n",
    "    \n",
    "    # Create the masked sentence by replacing the selected word with a special <mask> token or similar\n",
    "    words[mask_index] = '<mask>'\n",
    "    masked_sentence = ' '.join(words)\n",
    "    \n",
    "    return masked_sentence, masked_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "900752fe-c01f-489c-927a-fb60fde6d83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df34e8e-9a87-4b54-8e3c-2b7ecac2e74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "97f63289-5354-4779-8b3d-919706bb2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "train_sentences = [\"This is an example sentence.\", \"Here is another one.\", \"Machine learning is fun.\"]\n",
    "train_sentences = splited_data[0:10]\n",
    "\n",
    "word_to_token = {\"This\": 1, \"is\": 2, \"an\": 3, \"example\": 4, \"sentence\": 5, \"Here\": 6, \"another\": 7, \"one\": 8, \"Machine\": 9, \"learning\": 10, \"fun\": 11}\n",
    "word_to_token = token_map\n",
    "\n",
    "masked_sentences_and_targets = [mask_token_in_sentence(sentence, word_to_token) for sentence in train_sentences]\n",
    "\n",
    "# Filter out any None values if the word was not found in word_to_token\n",
    "masked_sentences_and_targets = [item for item in masked_sentences_and_targets if item[0] is not None and item[1] is not None]\n",
    "\n",
    "# Now, masked_sentences_and_targets contains tuples of (masked_sentence, masked_token)\n",
    "# You can split this into training and validation sets as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "bf5f95f5-5b69-469a-a86a-11f4b3113374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<mask> object that has a no-return boundary for other uses see black hole (disambiguation) end-of-sentence',\n",
       "  'start-of-sentence'),\n",
       " ('start-of-sentence direct radio image of a supermassive black hole at the core of messier 87 animated simulation of a schwarzschild <mask> hole with a galaxy passing behind end-of-sentence',\n",
       "  'black'),\n",
       " ('start-of-sentence around the time of alignment extreme gravitational lensing of the <mask> is observed end-of-sentence',\n",
       "  'galaxy'),\n",
       " ('start-of-sentence a black hole is a region of <mask> where gravity is so strong that nothing including light and other electromagnetic waves has enough energy to escape it end-of-sentence',\n",
       "  'spacetime'),\n",
       " ('start-of-sentence einstein s theory of general relativity predicts that a sufficiently compact mass can deform spacetime <mask> form a black hole end-of-sentence',\n",
       "  'to'),\n",
       " ('start-of-sentence the boundary of no escape is <mask> the event horizon end-of-sentence',\n",
       "  'called'),\n",
       " ('start-of-sentence <mask> black hole has a great effect on the fate and circumstances of an object crossing it but it has no locally detectable features according to general relativity end-of-sentence',\n",
       "  'a'),\n",
       " ('start-of-sentence in many <mask> a black hole acts like an ideal black body as it reflects no light end-of-sentence',\n",
       "  'ways'),\n",
       " ('start-of-sentence moreover quantum <mask> theory in curved spacetime predicts that event horizons emit hawking radiation with the same spectrum as a black body of a temperature inversely proportional to its mass end-of-sentence',\n",
       "  'field'),\n",
       " ('start-of-sentence this temperature is of the order of billionths of a kelvin for <mask> black holes making it essentially impossible to observe directly end-of-sentence',\n",
       "  'stellar')]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_sentences_and_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "da7dcea5-67f0-4cd8-9196-d2601f22085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_token_wrapper(word: str) -> int:\n",
    "    if word not in word_to_token:\n",
    "        return 0\n",
    "    return word_to_token[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "c5b94f5d-8552-4543-bf57-bf24e23f8c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_token(sentence)->int:\n",
    "\n",
    "    \n",
    "    return [word_to_token_wrapper(word) for word in sentence.replace(\"<mask> \",\"\").split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "7472f947-847c-469c-8a87-055a27199b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_sentences_and_targets_tokens = [(sentence_to_token(sentence),word_to_token[masked_word]) for sentence, masked_word in masked_sentences_and_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "35f39352-8441-4a16-89e5-768e5a1a3265",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([37, 10, 66, 6, 0, 186, 16, 68, 0, 182, 5, 8, 0, 3], 4),\n",
       " ([4,\n",
       "   172,\n",
       "   447,\n",
       "   125,\n",
       "   2,\n",
       "   6,\n",
       "   53,\n",
       "   5,\n",
       "   8,\n",
       "   36,\n",
       "   1,\n",
       "   750,\n",
       "   2,\n",
       "   586,\n",
       "   488,\n",
       "   0,\n",
       "   0,\n",
       "   2,\n",
       "   6,\n",
       "   64,\n",
       "   8,\n",
       "   32,\n",
       "   6,\n",
       "   141,\n",
       "   870,\n",
       "   713,\n",
       "   3],\n",
       "  5),\n",
       " ([4, 114, 1, 67, 2, 1085, 780, 27, 213, 2, 1, 12, 241, 3], 141),\n",
       " ([4,\n",
       "   6,\n",
       "   5,\n",
       "   8,\n",
       "   12,\n",
       "   6,\n",
       "   160,\n",
       "   2,\n",
       "   184,\n",
       "   61,\n",
       "   12,\n",
       "   130,\n",
       "   149,\n",
       "   10,\n",
       "   598,\n",
       "   813,\n",
       "   43,\n",
       "   11,\n",
       "   68,\n",
       "   333,\n",
       "   223,\n",
       "   66,\n",
       "   268,\n",
       "   106,\n",
       "   7,\n",
       "   137,\n",
       "   19,\n",
       "   3],\n",
       "  93),\n",
       " ([4,\n",
       "   105,\n",
       "   40,\n",
       "   59,\n",
       "   2,\n",
       "   48,\n",
       "   58,\n",
       "   438,\n",
       "   10,\n",
       "   6,\n",
       "   1983,\n",
       "   104,\n",
       "   20,\n",
       "   33,\n",
       "   757,\n",
       "   93,\n",
       "   140,\n",
       "   6,\n",
       "   5,\n",
       "   8,\n",
       "   3],\n",
       "  7)]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_sentences_and_targets_tokens[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5e773-d761-448e-8abb-3b5259b5b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    " ([37, 10, 66, 8, 0, 3], 4),\n",
    " ([4, 114, 1, 67, 2, 1085, 1, 12, 241, 3], 141)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad953b0-51be-4cf9-bc8a-79148bfbd4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8fcbbd-99da-4e21-b133-8d91a3fb96c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "b67fbcdd-8380-4423-a6f2-902d4b3bdc22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([4,\n",
       "   852,\n",
       "   51,\n",
       "   59,\n",
       "   9,\n",
       "   523,\n",
       "   93,\n",
       "   438,\n",
       "   10,\n",
       "   26,\n",
       "   412,\n",
       "   193,\n",
       "   57,\n",
       "   52,\n",
       "   32,\n",
       "   1,\n",
       "   120,\n",
       "   369,\n",
       "   14,\n",
       "   6,\n",
       "   5,\n",
       "   170,\n",
       "   2,\n",
       "   6,\n",
       "   94,\n",
       "   567,\n",
       "   247,\n",
       "   7,\n",
       "   63,\n",
       "   20,\n",
       "   3],\n",
       "  90),\n",
       " ([4,\n",
       "   172,\n",
       "   447,\n",
       "   125,\n",
       "   2,\n",
       "   6,\n",
       "   53,\n",
       "   5,\n",
       "   8,\n",
       "   36,\n",
       "   1,\n",
       "   750,\n",
       "   2,\n",
       "   586,\n",
       "   488,\n",
       "   0,\n",
       "   0,\n",
       "   2,\n",
       "   6,\n",
       "   64,\n",
       "   8,\n",
       "   32,\n",
       "   6,\n",
       "   141,\n",
       "   870,\n",
       "   713,\n",
       "   3],\n",
       "  5)]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "13c4289c-5698-4bf3-a2fb-125081853226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming train_data is a list of sentences\n",
    "train_sentences, val_sentences = train_test_split(masked_sentences_and_targets_tokens, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now, apply the masking and preparation logic to both train_sentences and val_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a1cc46-3182-443f-83d5-c5ede261ef8d",
   "metadata": {},
   "source": [
    "# Simple Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "23d000d7-be47-4ade-89e8-32eae9d3256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(SimpleLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        embedded = self.embedding(tokens)  # [batch_size, seq_len, embedding_dim]\n",
    "        # Aggregate embeddings, e.g., by averaging\n",
    "        aggregated = embedded.mean(dim=1)  # [batch_size, embedding_dim]\n",
    "        hidden = self.relu(self.linear1(aggregated))  # [batch_size, hidden_dim]\n",
    "        output = self.linear2(hidden)  # [batch_size, output_dim]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "3d9af6f7-4b2b-4ccb-be5d-033c001a3685",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = masked_sentences_and_targets_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "3a165738-3589-4bd2-8845-9faab9b7cd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 27, 14, 28, 21, 11, 29, 18, 31, 23]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(d) for d,_ in data] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "2dc2e5b9-693b-4b07-815c-2dd7405cc466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens, target = self.data[idx]\n",
    "        return torch.tensor(tokens, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "# Assuming `data` is your list of mappings\n",
    "dataset = TokenDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "877c5405-b7b0-4878-a9cb-ee8dff56ff6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2145"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "e792b000-fb49-4233-896a-ad07d5c0602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tokens, targets = zip(*batch)\n",
    "    # Pad the sequences to the maximum length in the batch\n",
    "    tokens_padded = pad_sequence([torch.tensor(seq) for seq in tokens], batch_first=True, padding_value=0)\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "    return tokens_padded, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "b9c33ac6-3879-443e-b91a-6695b9a02352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = 2145  # Adjust based on your dataset\n",
    "embedding_dim = 50\n",
    "hidden_dim = 100\n",
    "output_dim = vocab_size  # Same as vocab size for prediction\n",
    "\n",
    "model = SimpleLM(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "b743c574-5de1-47f2-8654-e3a271809ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "5c54399d-e1b1-4fef-ab9e-977967a24b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TokenDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "965f244d-8d83-49e0-9e20-3590a5b41fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/bv3z08h91lggkmhcp2n7yzh80000gn/T/ipykernel_59605/763346120.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens_padded = pad_sequence([torch.tensor(seq) for seq in tokens], batch_first=True, padding_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.690216700236003\n",
      "Epoch 2, Loss: 7.612291018168132\n",
      "Epoch 3, Loss: 7.557553609212239\n",
      "Epoch 4, Loss: 7.496306737263997\n",
      "Epoch 5, Loss: 7.440363725026448\n",
      "Epoch 6, Loss: 7.3489430745442705\n",
      "Epoch 7, Loss: 7.268342018127441\n",
      "Epoch 8, Loss: 7.243323644002278\n",
      "Epoch 9, Loss: 7.117378234863281\n",
      "Epoch 10, Loss: 7.0059231122334795\n",
      "Epoch 11, Loss: 6.845883369445801\n",
      "Epoch 12, Loss: 6.726955572764079\n",
      "Epoch 13, Loss: 6.531888961791992\n",
      "Epoch 14, Loss: 6.190521240234375\n",
      "Epoch 15, Loss: 6.126465638478597\n",
      "Epoch 16, Loss: 5.853911399841309\n",
      "Epoch 17, Loss: 5.596406777699788\n",
      "Epoch 18, Loss: 5.668986479441325\n",
      "Epoch 19, Loss: 4.926265875498454\n",
      "Epoch 20, Loss: 4.792790571848552\n",
      "Epoch 21, Loss: 4.143568197886149\n",
      "Epoch 22, Loss: 3.7135039965311685\n",
      "Epoch 23, Loss: 3.68514076868693\n",
      "Epoch 24, Loss: 3.4160588582356772\n",
      "Epoch 25, Loss: 3.0346622467041016\n",
      "Epoch 26, Loss: 2.7496185302734375\n",
      "Epoch 27, Loss: 2.7707616488138833\n",
      "Epoch 28, Loss: 2.5804885228474936\n",
      "Epoch 29, Loss: 2.4217917919158936\n",
      "Epoch 30, Loss: 2.2916815280914307\n",
      "Epoch 31, Loss: 2.101919492085775\n",
      "Epoch 32, Loss: 1.966813325881958\n",
      "Epoch 33, Loss: 1.8212985595067341\n",
      "Epoch 34, Loss: 1.8049161831537883\n",
      "Epoch 35, Loss: 1.6777344544728596\n",
      "Epoch 36, Loss: 1.6580937306086223\n",
      "Epoch 37, Loss: 1.632808248202006\n",
      "Epoch 38, Loss: 1.486732006072998\n",
      "Epoch 39, Loss: 1.5551708141962688\n",
      "Epoch 40, Loss: 1.5966306527455647\n",
      "Epoch 41, Loss: 1.4349550008773804\n",
      "Epoch 42, Loss: 1.51940123240153\n",
      "Epoch 43, Loss: 1.3172154029210408\n",
      "Epoch 44, Loss: 1.3066205978393555\n",
      "Epoch 45, Loss: 1.395201325416565\n",
      "Epoch 46, Loss: 1.3549530506134033\n",
      "Epoch 47, Loss: 1.2497724294662476\n",
      "Epoch 48, Loss: 1.1962477366129558\n",
      "Epoch 49, Loss: 1.1517958243687947\n",
      "Epoch 50, Loss: 1.1556929349899292\n",
      "Epoch 51, Loss: 1.0366727511088054\n",
      "Epoch 52, Loss: 1.1076003710428874\n",
      "Epoch 53, Loss: 1.148976723353068\n",
      "Epoch 54, Loss: 1.0622584422429402\n",
      "Epoch 55, Loss: 1.0452684164047241\n",
      "Epoch 56, Loss: 0.9379945198694865\n",
      "Epoch 57, Loss: 0.9590432643890381\n",
      "Epoch 58, Loss: 1.1182845036188762\n",
      "Epoch 59, Loss: 1.0054301023483276\n",
      "Epoch 60, Loss: 0.9460731546084086\n",
      "Epoch 61, Loss: 0.8756864666938782\n",
      "Epoch 62, Loss: 0.8453745245933533\n",
      "Epoch 63, Loss: 1.2683514753977458\n",
      "Epoch 64, Loss: 0.8027657866477966\n",
      "Epoch 65, Loss: 0.809607744216919\n",
      "Epoch 66, Loss: 0.786953886349996\n",
      "Epoch 67, Loss: 0.9420864383379618\n",
      "Epoch 68, Loss: 0.7781706849733988\n",
      "Epoch 69, Loss: 0.7038032015164694\n",
      "Epoch 70, Loss: 1.0772653818130493\n",
      "Epoch 71, Loss: 0.8297636310259501\n",
      "Epoch 72, Loss: 0.6716188788414001\n",
      "Epoch 73, Loss: 0.7846928040186564\n",
      "Epoch 74, Loss: 0.675011952718099\n",
      "Epoch 75, Loss: 0.7027330001195272\n",
      "Epoch 76, Loss: 0.6143852472305298\n",
      "Epoch 77, Loss: 1.0108214219411213\n",
      "Epoch 78, Loss: 0.6137648820877075\n",
      "Epoch 79, Loss: 0.5505231420199076\n",
      "Epoch 80, Loss: 0.5784087777137756\n",
      "Epoch 81, Loss: 0.800036778052648\n",
      "Epoch 82, Loss: 0.5122046073277792\n",
      "Epoch 83, Loss: 0.5296797553698221\n",
      "Epoch 84, Loss: 0.6411144534746805\n",
      "Epoch 85, Loss: 0.4906105697154999\n",
      "Epoch 86, Loss: 0.5113784869511923\n",
      "Epoch 87, Loss: 0.46790508429209393\n",
      "Epoch 88, Loss: 0.44576801856358844\n",
      "Epoch 89, Loss: 0.4221671024958293\n",
      "Epoch 90, Loss: 0.4230680763721466\n",
      "Epoch 91, Loss: 0.41328298052151996\n",
      "Epoch 92, Loss: 0.4709094564119975\n",
      "Epoch 93, Loss: 0.3879653016726176\n",
      "Epoch 94, Loss: 0.37993459900220233\n",
      "Epoch 95, Loss: 0.35134196281433105\n",
      "Epoch 96, Loss: 0.3819012939929962\n",
      "Epoch 97, Loss: 0.36552155017852783\n",
      "Epoch 98, Loss: 0.36891742547353107\n",
      "Epoch 99, Loss: 0.5822466611862183\n",
      "Epoch 100, Loss: 0.35772694150606793\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100  # Or whatever suits your dataset\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for tokens, target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(tokens)\n",
    "        loss = criterion(output, target.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8336ff6-f37d-4f6e-9435-e7e2077c97b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "f8d13c1b-cf93-4b0c-b77d-f6490ec614cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = sentence_to_token(\"black hole is the biggest object in the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "95c8979f-8994-49c7-abd2-3e397a88dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sequence\n",
    "#sequence = [37, 10, 66, 8, 0, 3]  # assuming this sequence with the masked token\n",
    "sequence_tensor = torch.tensor(sequence).unsqueeze(0)  # Add batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "29564edd-29ce-4ed8-b2b0-b1636217e5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    output = model(sequence_tensor)\n",
    "    predicted_token_id = output.argmax(dim=1).item()  # Get the index of the max log-probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "bebf6167-04fb-401e-bba6-a1c25345d4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a token_to_word dictionary\n",
    "token_to_word = {token: word for word, token in word_to_token.items()}\n",
    "predicted_word = token_to_word[predicted_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "21f74e0c-d6e9-4c0e-aa60-1ffb7dcf1845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'black'"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f40dd-b108-4b20-932d-4eab3a081bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "d95c8efc-8d55-4c47-9f19-64ac5160607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_missing_word(sentence=\"black hole is the biggest object in the\"):\n",
    "    sequence = sentence_to_token(sentence)\n",
    "    # Example sequence\n",
    "    #sequence = [37, 10, 66, 8, 0, 3]  # assuming this sequence with the masked token\n",
    "    sequence_tensor = torch.tensor(sequence).unsqueeze(0)  # Add batch dimension\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        output = model(sequence_tensor)\n",
    "        predicted_token_id = output.argmax(dim=1).item()  # Get the index of the max log-probability\n",
    "    # Assuming you have a token_to_word dictionary\n",
    "    token_to_word = {token: word for word, token in word_to_token.items()}\n",
    "    predicted_word = token_to_word[predicted_token_id]\n",
    "    return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "e438741a-664f-4f89-92eb-75b605d7d5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_missing_word(\"start-of-sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "88865d5f-ec41-4bd9-88c5-361a1378cc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start-of-sentence a black a black a black a black a black a black a black a black a black a'"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_word = \"start-of-sentence\"\n",
    "words = []\n",
    "for _ in range(20):\n",
    "    words.append(next_word)\n",
    "    next_word = predict_missing_word(next_word)\n",
    "\n",
    "\" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "1355625f-f15c-48a6-a54f-ed755c60708e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stellar'"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_missing_word(\"all black holes are\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1900def-8aa5-4fa6-9895-14994404a87d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3849cd5c-9708-41df-bcea-04d77d1e29f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
