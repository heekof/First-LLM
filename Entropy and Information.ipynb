{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is the qualtification of level of disorder. \n",
    "\n",
    "a High Entropy means high disorder.\n",
    "\n",
    "In information theory, the less entropy you have the more information you have.\n",
    "\n",
    "At initialization, a ANN is at high state of entropy. When training, the ANN weights are at low entropy whichmeans they managed to learn information from data.\n",
    "\n",
    "the formula and link between entropy and information is :\n",
    "\n",
    "Information = - log2(P) // with P the probability of an event occuring\n",
    "// if P=1 the evnent is certain and we will  no gain any Information from it so the Information = 0\n",
    "// P = 0 is impossible and the Information resulting is infinite, which mean it will requestion our understanding and world modelm\n",
    "\n",
    "We use the log2 to compute the information in bit. if p=0.5 ==> -log2(0.5)=1 , we need one bit to encode this message.\n",
    "\n",
    "\n",
    "Also information is a measure of difference between 2 entropies.\n",
    "Entropy can also tell us what is the maximum information we can learn about a system.\n",
    "The second law of thermodynamic says that entropy is increasing.\n",
    "\n",
    "Cross entropy = -SUM(Pk.log2(Pk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.8112781244591328)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(probabilities, base=2):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a distribution given its probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - probabilities: array-like, list of probabilities of outcomes.\n",
    "    - base: logarithm base, default is 2 for entropy in bits.\n",
    "\n",
    "    Returns:\n",
    "    - Entropy of the distribution.\n",
    "    \"\"\"\n",
    "    probabilities = np.array(probabilities)  # Ensure input is numpy array\n",
    "    # Filter out zero probabilities to avoid log(0)\n",
    "    probabilities = probabilities[probabilities > 0]\n",
    "    return -np.sum(probabilities * np.log(probabilities) / np.log(base))\n",
    "\n",
    "# Examples\n",
    "\n",
    "# Fair coin\n",
    "fair_coin = [0.5, 0.5]\n",
    "entropy_fair_coin = entropy(fair_coin)\n",
    "\n",
    "# Biased coin\n",
    "biased_coin = [0.75, 0.25]\n",
    "entropy_biased_coin = entropy(biased_coin)\n",
    "\n",
    "entropy_fair_coin, entropy_biased_coin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.8464393446710154, 0.8754887502163471)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def joint_entropy(distribution, base=2):\n",
    "    \"\"\"\n",
    "    Calculate the joint entropy of a 2-dimensional distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - distribution: 2D array-like, joint distribution probabilities of outcomes.\n",
    "    - base: logarithm base, default is 2 for entropy in bits.\n",
    "\n",
    "    Returns:\n",
    "    - Joint entropy of the distribution.\n",
    "    \"\"\"\n",
    "    distribution = np.array(distribution)\n",
    "    # Filter out zero probabilities to avoid log(0)\n",
    "    non_zero_probs = distribution[distribution > 0]\n",
    "    return -np.sum(non_zero_probs * np.log(non_zero_probs) / np.log(base))\n",
    "\n",
    "def conditional_entropy(joint_distribution, marginal_y, base=2):\n",
    "    \"\"\"\n",
    "    Calculate the conditional entropy H(X|Y) given a joint distribution of X and Y and the marginal distribution of Y.\n",
    "\n",
    "    Parameters:\n",
    "    - joint_distribution: 2D array-like, joint distribution probabilities of X and Y.\n",
    "    - marginal_y: array-like, marginal probabilities of Y.\n",
    "    - base: logarithm base, default is 2 for entropy in bits.\n",
    "\n",
    "    Returns:\n",
    "    - Conditional entropy of X given Y.\n",
    "    \"\"\"\n",
    "    joint_distribution = np.array(joint_distribution)\n",
    "    marginal_y = np.array(marginal_y)\n",
    "    conditional_probs = joint_distribution / marginal_y\n",
    "    # Filter out zero probabilities to avoid division by zero and log(0)\n",
    "    valid_probs = conditional_probs[marginal_y > 0]\n",
    "    return -np.sum(joint_distribution[marginal_y > 0] * np.log(valid_probs) / np.log(base))\n",
    "\n",
    "# Example: Joint and Conditional Entropy\n",
    "# Let's consider a simple example with a joint probability distribution\n",
    "joint_distribution = np.array([[0.1, 0.2], [0.3, 0.4]])  # P(X,Y)\n",
    "marginal_y = np.sum(joint_distribution, axis=0)  # Sum over X to get P(Y)\n",
    "\n",
    "# Calculate Joint Entropy H(X,Y)\n",
    "joint_entropy_value = joint_entropy(joint_distribution)\n",
    "\n",
    "# Calculate Conditional Entropy H(X|Y)\n",
    "conditional_entropy_value = conditional_entropy(joint_distribution, marginal_y)\n",
    "\n",
    "joint_entropy_value, conditional_entropy_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/bv3z08h91lggkmhcp2n7yzh80000gn/T/ipykernel_24158/159531598.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  joint_distribution = torch.tensor(joint_distribution, dtype=torch.float)\n",
      "/var/folders/l4/bv3z08h91lggkmhcp2n7yzh80000gn/T/ipykernel_24158/159531598.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  marginal_x = torch.tensor(marginal_x, dtype=torch.float).unsqueeze(1)  # Make marginal_x column vector\n",
      "/var/folders/l4/bv3z08h91lggkmhcp2n7yzh80000gn/T/ipykernel_24158/159531598.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  marginal_y = torch.tensor(marginal_y, dtype=torch.float).unsqueeze(0)  # Make marginal_y row vector\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0058)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def mutual_information(joint_distribution, marginal_x, marginal_y, base=2):\n",
    "    \"\"\"\n",
    "    Calculate the mutual information I(X;Y) given a joint distribution of X and Y and their marginal distributions.\n",
    "\n",
    "    Parameters:\n",
    "    - joint_distribution: 2D Tensor, joint distribution probabilities of X and Y.\n",
    "    - marginal_x: 1D Tensor, marginal probabilities of X.\n",
    "    - marginal_y: 1D Tensor, marginal probabilities of Y.\n",
    "    - base: logarithm base, default is 2 for entropy in bits.\n",
    "\n",
    "    Returns:\n",
    "    - Mutual information of X and Y.\n",
    "    \"\"\"\n",
    "    # Ensure inputs are tensors\n",
    "    joint_distribution = torch.tensor(joint_distribution, dtype=torch.float)\n",
    "    marginal_x = torch.tensor(marginal_x, dtype=torch.float).unsqueeze(1)  # Make marginal_x column vector\n",
    "    marginal_y = torch.tensor(marginal_y, dtype=torch.float).unsqueeze(0)  # Make marginal_y row vector\n",
    "\n",
    "    # Calculate the product of marginals for each pair (x, y)\n",
    "    product_of_marginals = marginal_x * marginal_y\n",
    "\n",
    "    # Calculate mutual information\n",
    "    # Use where to avoid log(0) and division by 0\n",
    "    valid_joint_probs = joint_distribution > 0\n",
    "    mutual_info = torch.where(valid_joint_probs,\n",
    "                              joint_distribution * torch.log2(joint_distribution / product_of_marginals),\n",
    "                              torch.zeros_like(joint_distribution))\n",
    "    \n",
    "    return mutual_info.sum()\n",
    "\n",
    "# Example: Mutual Information\n",
    "joint_distribution = torch.tensor([[0.1, 0.2], [0.3, 0.4]])\n",
    "marginal_x = torch.sum(joint_distribution, dim=1)  # Sum over Y to get P(X)\n",
    "marginal_y = torch.sum(joint_distribution, dim=0)  # Sum over X to get P(Y)\n",
    "\n",
    "# Calculate Mutual Information I(X;Y)\n",
    "mutual_information_value = mutual_information(joint_distribution, marginal_x, marginal_y)\n",
    "\n",
    "mutual_information_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
