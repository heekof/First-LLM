{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, my name is Jaafar and I love NLP\" #, because it allows us to capture syntactic and semantic from natural language.\"\n",
    "\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text.split(\" \"))))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "unique_words = sorted(list(set(text)))\n",
    "\n",
    "tokens_text = enc.encode(text)\n",
    "\n",
    "vocab_size = len(tokens_text)\n",
    "\n",
    "\n",
    "jaafar_tokens = {i:token for i, token in enumerate(tokens_text)}\n",
    "jaafar_tokens_reversed = {token:i for i, token in enumerate(tokens_text)}\n",
    "\n",
    "\n",
    "encode = lambda s: [jaafar_tokens_reversed[c] for c in enc.encode(s)] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([enc.decode([jaafar_tokens[i]]) for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "\n",
    "example = \"Hello, my name is Jaafar\"\n",
    "assert decode(encode(example)) == example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 9906,\n",
       " 1: 11,\n",
       " 2: 856,\n",
       " 3: 836,\n",
       " 4: 374,\n",
       " 5: 23720,\n",
       " 6: 2642,\n",
       " 7: 277,\n",
       " 8: 323,\n",
       " 9: 358,\n",
       " 10: 3021,\n",
       " 11: 452,\n",
       " 12: 12852}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaafar_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9906: 0,\n",
       " 11: 1,\n",
       " 856: 2,\n",
       " 836: 3,\n",
       " 374: 4,\n",
       " 23720: 5,\n",
       " 2642: 6,\n",
       " 277: 7,\n",
       " 323: 8,\n",
       " 358: 9,\n",
       " 3021: 10,\n",
       " 452: 11,\n",
       " 12852: 12}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaafar_tokens_reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is Jaafar and I love NLP\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "9\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "print(encode(text))\n",
    "print(len(text.split()))\n",
    "print(len(tokens_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # TODO : draw random number from 0 to [len(data)-block_size], in just one dimension. And we want N=batch_size element\n",
    "    # We want to select randomly N=batch_size times a block size ! \n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    # TODO : We draw from this random generate our block size = Context size.\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "\n",
    "    # we take the next block size as a prediction\n",
    "    # We just add one so that for every index, the prediction is at the same row in the same colum \n",
    "    # from X context 0 - N in a given row X  => prediction colum N+1 in y\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 # How many independent sequence we will process in parallel ?\n",
    "block_size = 5 # How is the maximum context length N\n",
    "\n",
    "train_data = data\n",
    "X, y = get_batch(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data =  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
      "block_size =  5\n",
      "batch_size =  4\n",
      "8 (4,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3, 5, 5, 0])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = train_data\n",
    "a = len(data) - block_size\n",
    "b = (batch_size,)\n",
    "print(\"data = \",data)\n",
    "print(\"block_size = \", block_size)\n",
    "print(\"batch_size = \", batch_size)\n",
    "print(a,b)\n",
    "torch.randint(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 2, 3, 4, 5]),\n",
       " tensor([1, 2, 3, 4, 5]),\n",
       " tensor([1, 2, 3, 4, 5]),\n",
       " tensor([4, 5, 6, 7, 8])]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data[i:i+block_size] for i in [1, 1, 1, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([2, 3, 4, 5, 6]),\n",
       " tensor([2, 3, 4, 5, 6]),\n",
       " tensor([2, 3, 4, 5, 6]),\n",
       " tensor([5, 6, 7, 8, 9])]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data[i+1:i+block_size+1] for i in [1, 1, 1, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function randint in module torch:\n",
      "\n",
      "randint(...)\n",
      "    randint(low=0, high, size, \\*, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "    \n",
      "    Returns a tensor filled with random integers generated uniformly\n",
      "    between :attr:`low` (inclusive) and :attr:`high` (exclusive).\n",
      "    \n",
      "    The shape of the tensor is defined by the variable argument :attr:`size`.\n",
      "    \n",
      "    .. note::\n",
      "        With the global dtype default (``torch.float32``), this function returns\n",
      "        a tensor with dtype ``torch.int64``.\n",
      "    \n",
      "    Args:\n",
      "        low (int, optional): Lowest integer to be drawn from the distribution. Default: 0.\n",
      "        high (int): One above the highest integer to be drawn from the distribution.\n",
      "        size (tuple): a tuple defining the shape of the output tensor.\n",
      "    \n",
      "    Keyword args:\n",
      "        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n",
      "        out (Tensor, optional): the output tensor.\n",
      "        dtype (`torch.dtype`, optional) - the desired data type of returned tensor. Default: if ``None``,\n",
      "            this function returns a tensor with dtype ``torch.int64``.\n",
      "        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "            Default: ``torch.strided``.\n",
      "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "            Default: if ``None``, uses the current device for the default tensor type\n",
      "            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "            for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "        requires_grad (bool, optional): If autograd should record operations on the\n",
      "            returned tensor. Default: ``False``.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.randint(3, 5, (3,))\n",
      "        tensor([4, 3, 4])\n",
      "    \n",
      "    \n",
      "        >>> torch.randint(10, (2, 2))\n",
      "        tensor([[0, 2],\n",
      "                [5, 5]])\n",
      "    \n",
      "    \n",
      "        >>> torch.randint(3, 10, (2, 2))\n",
      "        tensor([[4, 5],\n",
      "                [6, 7]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.randint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 4])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(block_size, (batch_size,))\n",
    "# ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  5,  6,  7,  8],\n",
      "        [ 1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10],\n",
      "        [ 2,  3,  4,  5,  6]])\n"
     ]
    }
   ],
   "source": [
    "X, y = get_batch(\"train\")\n",
    "\n",
    "print(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [ 2,  3,  4,  5,  6],\n",
      "        [ 7,  8,  9, 10, 11],\n",
      "        [ 3,  4,  5,  6,  7]])\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  tensor([4])  ==> y =  tensor(5)\n",
      "X =  tensor([4, 5])  ==> y =  tensor(6)\n",
      "X =  tensor([4, 5, 6])  ==> y =  tensor(7)\n",
      "X =  tensor([4, 5, 6, 7])  ==> y =  tensor(8)\n",
      "X =  tensor([4, 5, 6, 7, 8])  ==> y =  tensor(9)\n",
      "X =  tensor([1])  ==> y =  tensor(2)\n",
      "X =  tensor([1, 2])  ==> y =  tensor(3)\n",
      "X =  tensor([1, 2, 3])  ==> y =  tensor(4)\n",
      "X =  tensor([1, 2, 3, 4])  ==> y =  tensor(5)\n",
      "X =  tensor([1, 2, 3, 4, 5])  ==> y =  tensor(6)\n",
      "X =  tensor([6])  ==> y =  tensor(7)\n",
      "X =  tensor([6, 7])  ==> y =  tensor(8)\n",
      "X =  tensor([6, 7, 8])  ==> y =  tensor(9)\n",
      "X =  tensor([6, 7, 8, 9])  ==> y =  tensor(10)\n",
      "X =  tensor([ 6,  7,  8,  9, 10])  ==> y =  tensor(11)\n",
      "X =  tensor([2])  ==> y =  tensor(3)\n",
      "X =  tensor([2, 3])  ==> y =  tensor(4)\n",
      "X =  tensor([2, 3, 4])  ==> y =  tensor(5)\n",
      "X =  tensor([2, 3, 4, 5])  ==> y =  tensor(6)\n",
      "X =  tensor([2, 3, 4, 5, 6])  ==> y =  tensor(7)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X)):\n",
    "    for j in range(len(row)):\n",
    "        \n",
    "        print(\"X = \", X[i,:j+1], \" ==> y = \", y[i,j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  5,  6,  7,  8],\n",
       "        [ 1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10],\n",
       "        [ 2,  3,  4,  5,  6]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape # row, colums ! BATCH, CONTEXT (time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F \n",
    "torch.manual_seed(69)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token will be projected into a 13*13 dimension\n",
    "        # Given a context of words, what is its meaning in our world (vocab dimension)\n",
    "        # what is also happening befin the scene is that each token is encode into a one hot encoder before feedforward\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "\n",
    "        # row, colums ! BATCH, CONTEXT (time)\n",
    "        logits = self.token_embedding_table(x) # BATCH, CONTEXT, VOCAB_SIZE = Channel !\n",
    "\n",
    "        loss = F.cross_entropy(logits, targets) # it want a B,T,C instead of a B,C,T\n",
    "        \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 4,  5,  6,  7,  8],\n",
       "         [ 1,  2,  3,  4,  5],\n",
       "         [ 6,  7,  8,  9, 10],\n",
       "         [ 2,  3,  4,  5,  6]]),\n",
       " tensor([[ 5,  6,  7,  8,  9],\n",
       "         [ 2,  3,  4,  5,  6],\n",
       "         [ 7,  8,  9, 10, 11],\n",
       "         [ 3,  4,  5,  6,  7]]))"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [4, 13], got [4, 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[238], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m m \u001b[38;5;241m=\u001b[39m BigramLanguageModel(vocab_size\u001b[38;5;241m=\u001b[39mvocab_size)\n\u001b[0;32m----> 3\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m m(X,y)\n\u001b[1;32m      5\u001b[0m logits\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/envs/env_pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/env_pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[236], line 20\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[0;34m(self, x, targets)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, targets):\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# row, colums ! BATCH, CONTEXT (time)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding_table(x) \u001b[38;5;66;03m# BATCH, CONTEXT, VOCAB_SIZE = Channel !\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, targets)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits, loss\n",
      "File \u001b[0;32m~/anaconda3/envs/env_pytorch/lib/python3.11/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [4, 13], got [4, 5]"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size=vocab_size)\n",
    "\n",
    "logits, loss = m(X,y)\n",
    "\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8756,  0.2960,  0.6394,  1.2406, -1.2100, -0.9481,  0.6444,\n",
       "           0.2188, -1.2224, -0.9322, -0.3832,  1.4027,  0.4913],\n",
       "         [ 1.0152, -0.0184, -0.8487,  1.8939, -0.0325,  0.3191, -0.5700,\n",
       "          -0.4292, -1.0216, -0.5285,  1.6272, -0.2411, -2.1021],\n",
       "         [-0.0179,  0.2241,  0.4389,  1.6109, -0.7284, -1.6016,  0.4660,\n",
       "          -0.1961,  0.3063, -1.8420, -0.4396,  1.3367,  0.3671],\n",
       "         [ 1.3794,  1.1527,  0.0762,  0.0557, -0.6163, -1.2772,  0.4905,\n",
       "          -0.4098,  0.0517,  0.7329, -0.7448,  2.2900,  0.1214],\n",
       "         [-1.3227,  1.0725, -1.6382,  1.5207,  0.5975,  0.5968, -0.5111,\n",
       "          -0.2617, -0.4817,  0.4692,  0.4951, -0.2140, -1.0078]],\n",
       "\n",
       "        [[ 0.3527, -0.0916,  0.0341, -0.8986,  0.1022, -0.6627, -0.1350,\n",
       "          -0.3983, -1.7892,  1.2785,  1.3351, -0.3066,  1.0382],\n",
       "         [ 1.2762,  0.0419, -1.2794, -1.8432,  0.8633, -1.7786, -0.8080,\n",
       "          -0.8735,  0.9367, -1.2319,  1.5287, -0.2759, -0.8625],\n",
       "         [-0.1915, -0.4807, -1.4154,  0.0934, -0.2420, -1.0300, -0.2034,\n",
       "          -0.6882, -0.0178,  1.1983, -0.8180, -0.7297,  0.8256],\n",
       "         [ 0.8756,  0.2960,  0.6394,  1.2406, -1.2100, -0.9481,  0.6444,\n",
       "           0.2188, -1.2224, -0.9322, -0.3832,  1.4027,  0.4913],\n",
       "         [ 1.0152, -0.0184, -0.8487,  1.8939, -0.0325,  0.3191, -0.5700,\n",
       "          -0.4292, -1.0216, -0.5285,  1.6272, -0.2411, -2.1021]],\n",
       "\n",
       "        [[-0.0179,  0.2241,  0.4389,  1.6109, -0.7284, -1.6016,  0.4660,\n",
       "          -0.1961,  0.3063, -1.8420, -0.4396,  1.3367,  0.3671],\n",
       "         [ 1.3794,  1.1527,  0.0762,  0.0557, -0.6163, -1.2772,  0.4905,\n",
       "          -0.4098,  0.0517,  0.7329, -0.7448,  2.2900,  0.1214],\n",
       "         [-1.3227,  1.0725, -1.6382,  1.5207,  0.5975,  0.5968, -0.5111,\n",
       "          -0.2617, -0.4817,  0.4692,  0.4951, -0.2140, -1.0078],\n",
       "         [ 0.8051,  0.7826, -0.8145, -1.7371,  0.4831,  0.4684, -0.3933,\n",
       "           0.2791,  0.8211,  0.6328,  0.8020, -1.0774, -1.1581],\n",
       "         [ 0.6102,  0.9207, -0.2210, -1.2322,  1.3270, -0.6450,  0.6563,\n",
       "          -0.4443,  0.0276,  2.1283,  0.0063,  0.5087, -1.4697]],\n",
       "\n",
       "        [[ 1.2762,  0.0419, -1.2794, -1.8432,  0.8633, -1.7786, -0.8080,\n",
       "          -0.8735,  0.9367, -1.2319,  1.5287, -0.2759, -0.8625],\n",
       "         [-0.1915, -0.4807, -1.4154,  0.0934, -0.2420, -1.0300, -0.2034,\n",
       "          -0.6882, -0.0178,  1.1983, -0.8180, -0.7297,  0.8256],\n",
       "         [ 0.8756,  0.2960,  0.6394,  1.2406, -1.2100, -0.9481,  0.6444,\n",
       "           0.2188, -1.2224, -0.9322, -0.3832,  1.4027,  0.4913],\n",
       "         [ 1.0152, -0.0184, -0.8487,  1.8939, -0.0325,  0.3191, -0.5700,\n",
       "          -0.4292, -1.0216, -0.5285,  1.6272, -0.2411, -2.1021],\n",
       "         [-0.0179,  0.2241,  0.4389,  1.6109, -0.7284, -1.6016,  0.4660,\n",
       "          -0.1961,  0.3063, -1.8420, -0.4396,  1.3367,  0.3671]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 13])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4754, 2.4755, 2.3665, 3.1145, 2.5681],\n",
       "        [2.6159, 2.4354, 1.9926, 3.2199, 2.7362],\n",
       "        [2.5672, 2.9067, 2.2769, 2.4989, 2.7504],\n",
       "        [2.4330, 2.2624, 2.9257, 2.7202, 2.6587]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = logits.exp()\n",
    "\n",
    "probs = exp/torch.sum(exp, dim=1, keepdim=True)\n",
    "\n",
    "probs.sum(2)\n",
    "\n",
    "#probs.argmax(2, keepdim=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4754, 2.4755, 2.3665, 3.1145, 2.5681],\n",
       "        [2.6159, 2.4354, 1.9926, 3.2199, 2.7362],\n",
       "        [2.5672, 2.9067, 2.2769, 2.4989, 2.7504],\n",
       "        [2.4330, 2.2624, 2.9257, 2.7202, 2.6587]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(logits,1,dtype=torch.float32).sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  5,  6,  7,  8],\n",
       "        [ 1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10],\n",
       "        [ 2,  3,  4,  5,  6]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 13])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.5300, -1.3035,  0.4438,  1.2221,  1.0395,  0.9608,  0.4214,  0.7452,\n",
       "         -1.8389, -1.2497, -0.2485,  0.1428, -1.0509],\n",
       "        [ 0.3527, -0.0916,  0.0341, -0.8986,  0.1022, -0.6627, -0.1350, -0.3983,\n",
       "         -1.7892,  1.2785,  1.3351, -0.3066,  1.0382],\n",
       "        [ 1.2762,  0.0419, -1.2794, -1.8432,  0.8633, -1.7786, -0.8080, -0.8735,\n",
       "          0.9367, -1.2319,  1.5287, -0.2759, -0.8625],\n",
       "        [-0.1915, -0.4807, -1.4154,  0.0934, -0.2420, -1.0300, -0.2034, -0.6882,\n",
       "         -0.0178,  1.1983, -0.8180, -0.7297,  0.8256],\n",
       "        [ 0.8756,  0.2960,  0.6394,  1.2406, -1.2100, -0.9481,  0.6444,  0.2188,\n",
       "         -1.2224, -0.9322, -0.3832,  1.4027,  0.4913],\n",
       "        [ 1.0152, -0.0184, -0.8487,  1.8939, -0.0325,  0.3191, -0.5700, -0.4292,\n",
       "         -1.0216, -0.5285,  1.6272, -0.2411, -2.1021],\n",
       "        [-0.0179,  0.2241,  0.4389,  1.6109, -0.7284, -1.6016,  0.4660, -0.1961,\n",
       "          0.3063, -1.8420, -0.4396,  1.3367,  0.3671],\n",
       "        [ 1.3794,  1.1527,  0.0762,  0.0557, -0.6163, -1.2772,  0.4905, -0.4098,\n",
       "          0.0517,  0.7329, -0.7448,  2.2900,  0.1214],\n",
       "        [-1.3227,  1.0725, -1.6382,  1.5207,  0.5975,  0.5968, -0.5111, -0.2617,\n",
       "         -0.4817,  0.4692,  0.4951, -0.2140, -1.0078],\n",
       "        [ 0.8051,  0.7826, -0.8145, -1.7371,  0.4831,  0.4684, -0.3933,  0.2791,\n",
       "          0.8211,  0.6328,  0.8020, -1.0774, -1.1581],\n",
       "        [ 0.6102,  0.9207, -0.2210, -1.2322,  1.3270, -0.6450,  0.6563, -0.4443,\n",
       "          0.0276,  2.1283,  0.0063,  0.5087, -1.4697],\n",
       "        [ 0.6544, -0.1272, -1.0534,  0.9279, -0.1020, -1.0190,  1.2218, -0.3401,\n",
       "         -0.3922, -1.3434,  0.0306,  0.1347, -0.3442],\n",
       "        [-0.9858, -0.3640, -0.4063, -0.2708,  2.3422,  0.4314,  0.2432,  0.2693,\n",
       "          1.0670,  0.0287, -0.6572,  0.1181,  0.3076]], requires_grad=True)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weights = m.token_embedding_table.weight\n",
    "print(embedding_weights.shape)\n",
    "embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, my name is Jaafar and I love NLP'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  5,  6,  7,  8],\n",
       "        [ 1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10],\n",
       "        [ 2,  3,  4,  5,  6]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8756,  0.2960,  0.6394,  1.2406, -1.2100, -0.9481,  0.6444,\n",
       "           0.2188, -1.2224, -0.9322, -0.3832,  1.4027,  0.4913],\n",
       "         [ 1.0152, -0.0184, -0.8487,  1.8939, -0.0325,  0.3191, -0.5700,\n",
       "          -0.4292, -1.0216, -0.5285,  1.6272, -0.2411, -2.1021],\n",
       "         [-0.0179,  0.2241,  0.4389,  1.6109, -0.7284, -1.6016,  0.4660,\n",
       "          -0.1961,  0.3063, -1.8420, -0.4396,  1.3367,  0.3671],\n",
       "         [ 1.3794,  1.1527,  0.0762,  0.0557, -0.6163, -1.2772,  0.4905,\n",
       "          -0.4098,  0.0517,  0.7329, -0.7448,  2.2900,  0.1214],\n",
       "         [-1.3227,  1.0725, -1.6382,  1.5207,  0.5975,  0.5968, -0.5111,\n",
       "          -0.2617, -0.4817,  0.4692,  0.4951, -0.2140, -1.0078]],\n",
       "\n",
       "        [[ 0.3527, -0.0916,  0.0341, -0.8986,  0.1022, -0.6627, -0.1350,\n",
       "          -0.3983, -1.7892,  1.2785,  1.3351, -0.3066,  1.0382],\n",
       "         [ 1.2762,  0.0419, -1.2794, -1.8432,  0.8633, -1.7786, -0.8080,\n",
       "          -0.8735,  0.9367, -1.2319,  1.5287, -0.2759, -0.8625],\n",
       "         [-0.1915, -0.4807, -1.4154,  0.0934, -0.2420, -1.0300, -0.2034,\n",
       "          -0.6882, -0.0178,  1.1983, -0.8180, -0.7297,  0.8256],\n",
       "         [ 0.8756,  0.2960,  0.6394,  1.2406, -1.2100, -0.9481,  0.6444,\n",
       "           0.2188, -1.2224, -0.9322, -0.3832,  1.4027,  0.4913],\n",
       "         [ 1.0152, -0.0184, -0.8487,  1.8939, -0.0325,  0.3191, -0.5700,\n",
       "          -0.4292, -1.0216, -0.5285,  1.6272, -0.2411, -2.1021]],\n",
       "\n",
       "        [[-0.0179,  0.2241,  0.4389,  1.6109, -0.7284, -1.6016,  0.4660,\n",
       "          -0.1961,  0.3063, -1.8420, -0.4396,  1.3367,  0.3671],\n",
       "         [ 1.3794,  1.1527,  0.0762,  0.0557, -0.6163, -1.2772,  0.4905,\n",
       "          -0.4098,  0.0517,  0.7329, -0.7448,  2.2900,  0.1214],\n",
       "         [-1.3227,  1.0725, -1.6382,  1.5207,  0.5975,  0.5968, -0.5111,\n",
       "          -0.2617, -0.4817,  0.4692,  0.4951, -0.2140, -1.0078],\n",
       "         [ 0.8051,  0.7826, -0.8145, -1.7371,  0.4831,  0.4684, -0.3933,\n",
       "           0.2791,  0.8211,  0.6328,  0.8020, -1.0774, -1.1581],\n",
       "         [ 0.6102,  0.9207, -0.2210, -1.2322,  1.3270, -0.6450,  0.6563,\n",
       "          -0.4443,  0.0276,  2.1283,  0.0063,  0.5087, -1.4697]],\n",
       "\n",
       "        [[ 1.2762,  0.0419, -1.2794, -1.8432,  0.8633, -1.7786, -0.8080,\n",
       "          -0.8735,  0.9367, -1.2319,  1.5287, -0.2759, -0.8625],\n",
       "         [-0.1915, -0.4807, -1.4154,  0.0934, -0.2420, -1.0300, -0.2034,\n",
       "          -0.6882, -0.0178,  1.1983, -0.8180, -0.7297,  0.8256],\n",
       "         [ 0.8756,  0.2960,  0.6394,  1.2406, -1.2100, -0.9481,  0.6444,\n",
       "           0.2188, -1.2224, -0.9322, -0.3832,  1.4027,  0.4913],\n",
       "         [ 1.0152, -0.0184, -0.8487,  1.8939, -0.0325,  0.3191, -0.5700,\n",
       "          -0.4292, -1.0216, -0.5285,  1.6272, -0.2411, -2.1021],\n",
       "         [-0.0179,  0.2241,  0.4389,  1.6109, -0.7284, -1.6016,  0.4660,\n",
       "          -0.1961,  0.3063, -1.8420, -0.4396,  1.3367,  0.3671]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1127, -0.9366, -0.0756,  3.3023, -0.6855],\n",
       "        [-0.1412, -4.3060, -3.6993,  1.1127, -0.9366],\n",
       "        [-0.0756,  3.3023, -0.6855, -0.1063,  2.1730],\n",
       "        [-4.3060, -3.6993,  1.1127, -0.9366, -0.0756]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(out, dim=2, keepdim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
